Log dir data/mb_mpc_HalfCheetah-v1_10-01-2018_18-03-17 already exists! Delete it first or use a different dir
[32;1mLogging data to data/mb_mpc_HalfCheetah-v1_10-01-2018_18-03-17/log.txt[0m
-------- env info --------
observation_space:  (20,)
action_space:  (6,)
 
collecting random data .....  
random data iter  0
random data iter  1
random data iter  2
random data iter  3
random data iter  4
random data iter  5
random data iter  6
random data iter  7
random data iter  8
random data iter  9
data buffer size:  10000
onpol_iters:  0
paths_onpol:  0  running.....
data buffer size:  10000
paths_onpol:  1  running.....
data buffer size:  11000
paths_onpol:  2  running.....
data buffer size:  12000
paths_onpol:  3  running.....
data buffer size:  13000
paths_onpol:  4  running.....
data buffer size:  14000
paths_onpol:  5  running.....
data buffer size:  15000
paths_onpol:  6  running.....
data buffer size:  16000
paths_onpol:  7  running.....
data buffer size:  17000
paths_onpol:  8  running.....
data buffer size:  18000
paths_onpol:  9  running.....
data buffer size:  19000
-------------------------------------
|       Iteration |               0 |
|     AverageCost |        8.75e+03 |
|         StdCost |             445 |
|     MinimumCost |           8e+03 |
|     MaximumCost |        9.35e+03 |
|   AverageReturn |            -729 |
|       StdReturn |              67 |
|   MinimumReturn |            -844 |
|   MaximumReturn |            -620 |
-------------------------------------
onpol_iters:  1
paths_onpol:  0  running.....
data buffer size:  20000
paths_onpol:  1  running.....
data buffer size:  21000
paths_onpol:  2  running.....
data buffer size:  22000
paths_onpol:  3  running.....
data buffer size:  23000
paths_onpol:  4  running.....
data buffer size:  24000
paths_onpol:  5  running.....
data buffer size:  25000
paths_onpol:  6  running.....
data buffer size:  26000
paths_onpol:  7  running.....
data buffer size:  27000
paths_onpol:  8  running.....
data buffer size:  28000
paths_onpol:  9  running.....
data buffer size:  29000
-------------------------------------
|       Iteration |               1 |
|     AverageCost |        9.91e+03 |
|         StdCost |             539 |
|     MinimumCost |        8.95e+03 |
|     MaximumCost |        1.06e+04 |
|   AverageReturn |            -940 |
|       StdReturn |            76.8 |
|   MinimumReturn |       -1.05e+03 |
|   MaximumReturn |            -828 |
-------------------------------------
onpol_iters:  2
paths_onpol:  0  running.....
data buffer size:  30000
paths_onpol:  1  running.....
data buffer size:  31000
paths_onpol:  2  running.....
data buffer size:  32000
paths_onpol:  3  running.....
data buffer size:  33000
paths_onpol:  4  running.....
data buffer size:  34000
paths_onpol:  5  running.....
data buffer size:  35000
paths_onpol:  6  running.....
data buffer size:  36000
paths_onpol:  7  running.....
data buffer size:  37000
paths_onpol:  8  running.....
data buffer size:  38000
paths_onpol:  9  running.....
data buffer size:  39000
-------------------------------------
|       Iteration |               2 |
|     AverageCost |         9.3e+03 |
|         StdCost |             424 |
|     MinimumCost |        8.68e+03 |
|     MaximumCost |        9.88e+03 |
|   AverageReturn |            -799 |
|       StdReturn |            62.4 |
|   MinimumReturn |            -935 |
|   MaximumReturn |            -724 |
-------------------------------------
onpol_iters:  3
paths_onpol:  0  running.....
data buffer size:  40000
paths_onpol:  1  running.....
data buffer size:  41000
paths_onpol:  2  running.....
data buffer size:  42000
paths_onpol:  3  running.....
data buffer size:  43000
paths_onpol:  4  running.....
data buffer size:  44000
paths_onpol:  5  running.....
data buffer size:  45000
paths_onpol:  6  running.....
data buffer size:  46000
paths_onpol:  7  running.....
data buffer size:  47000
paths_onpol:  8  running.....
data buffer size:  48000
paths_onpol:  9  running.....
data buffer size:  49000
-------------------------------------
|       Iteration |               3 |
|     AverageCost |        1.01e+04 |
|         StdCost |             631 |
|     MinimumCost |        8.72e+03 |
|     MaximumCost |        1.12e+04 |
|   AverageReturn |            -882 |
|       StdReturn |            60.1 |
|   MinimumReturn |            -988 |
|   MaximumReturn |            -747 |
-------------------------------------
onpol_iters:  4
paths_onpol:  0  running.....
data buffer size:  50000
paths_onpol:  1  running.....
data buffer size:  51000
paths_onpol:  2  running.....
data buffer size:  52000
paths_onpol:  3  running.....
data buffer size:  53000
paths_onpol:  4  running.....
data buffer size:  54000
paths_onpol:  5  running.....
data buffer size:  55000
paths_onpol:  6  running.....
data buffer size:  56000
paths_onpol:  7  running.....
data buffer size:  57000
paths_onpol:  8  running.....
data buffer size:  58000
paths_onpol:  9  running.....
data buffer size:  59000
-------------------------------------
|       Iteration |               4 |
|     AverageCost |        6.71e+03 |
|         StdCost |             423 |
|     MinimumCost |        6.02e+03 |
|     MaximumCost |        7.21e+03 |
|   AverageReturn |            -669 |
|       StdReturn |            54.9 |
|   MinimumReturn |            -756 |
|   MaximumReturn |            -581 |
-------------------------------------
onpol_iters:  5
paths_onpol:  0  running.....
data buffer size:  60000
paths_onpol:  1  running.....
data buffer size:  61000
paths_onpol:  2  running.....
data buffer size:  62000
paths_onpol:  3  running.....
data buffer size:  63000
paths_onpol:  4  running.....
data buffer size:  64000
paths_onpol:  5  running.....
data buffer size:  65000
paths_onpol:  6  running.....
data buffer size:  66000
paths_onpol:  7  running.....
data buffer size:  67000
paths_onpol:  8  running.....
data buffer size:  68000
paths_onpol:  9  running.....
data buffer size:  69000
-------------------------------------
|       Iteration |               5 |
|     AverageCost |        7.66e+03 |
|         StdCost |             411 |
|     MinimumCost |        6.94e+03 |
|     MaximumCost |        8.42e+03 |
|   AverageReturn |            -709 |
|       StdReturn |             116 |
|   MinimumReturn |            -939 |
|   MaximumReturn |            -495 |
-------------------------------------
onpol_iters:  6
paths_onpol:  0  running.....
data buffer size:  70000
paths_onpol:  1  running.....
data buffer size:  71000
paths_onpol:  2  running.....
data buffer size:  72000
paths_onpol:  3  running.....
data buffer size:  73000
paths_onpol:  4  running.....
data buffer size:  74000
paths_onpol:  5  running.....
data buffer size:  75000
paths_onpol:  6  running.....
data buffer size:  76000
paths_onpol:  7  running.....
data buffer size:  77000
paths_onpol:  8  running.....
data buffer size:  78000
paths_onpol:  9  running.....
data buffer size:  79000
-------------------------------------
|       Iteration |               6 |
|     AverageCost |        8.66e+03 |
|         StdCost |             506 |
|     MinimumCost |        7.84e+03 |
|     MaximumCost |        9.76e+03 |
|   AverageReturn |            -867 |
|       StdReturn |            77.6 |
|   MinimumReturn |       -1.01e+03 |
|   MaximumReturn |            -756 |
-------------------------------------
onpol_iters:  7
paths_onpol:  0  running.....
data buffer size:  80000
paths_onpol:  1  running.....
data buffer size:  81000
paths_onpol:  2  running.....
data buffer size:  82000
paths_onpol:  3  running.....
data buffer size:  83000
paths_onpol:  4  running.....
data buffer size:  84000
paths_onpol:  5  running.....
data buffer size:  85000
paths_onpol:  6  running.....
data buffer size:  86000
paths_onpol:  7  running.....
data buffer size:  87000
paths_onpol:  8  running.....
data buffer size:  88000
paths_onpol:  9  running.....
data buffer size:  89000
-------------------------------------
|       Iteration |               7 |
|     AverageCost |        6.31e+03 |
|         StdCost |             427 |
|     MinimumCost |        5.82e+03 |
|     MaximumCost |        7.41e+03 |
|   AverageReturn |            -603 |
|       StdReturn |             101 |
|   MinimumReturn |            -824 |
|   MaximumReturn |            -468 |
-------------------------------------
onpol_iters:  8
paths_onpol:  0  running.....
data buffer size:  90000
paths_onpol:  1  running.....
data buffer size:  91000
paths_onpol:  2  running.....
data buffer size:  92000
paths_onpol:  3  running.....
data buffer size:  93000
paths_onpol:  4  running.....
data buffer size:  94000
paths_onpol:  5  running.....
data buffer size:  95000
paths_onpol:  6  running.....
data buffer size:  96000
paths_onpol:  7  running.....
data buffer size:  97000
paths_onpol:  8  running.....
data buffer size:  98000
paths_onpol:  9  running.....
data buffer size:  99000
-------------------------------------
|       Iteration |               8 |
|     AverageCost |        7.48e+03 |
|         StdCost |             551 |
|     MinimumCost |        6.43e+03 |
|     MaximumCost |        8.14e+03 |
|   AverageReturn |            -699 |
|       StdReturn |            76.2 |
|   MinimumReturn |            -836 |
|   MaximumReturn |            -610 |
-------------------------------------
onpol_iters:  9
paths_onpol:  0  running.....
data buffer size:  100000
paths_onpol:  1  running.....
data buffer size:  101000
paths_onpol:  2  running.....
data buffer size:  102000
paths_onpol:  3  running.....
data buffer size:  103000
paths_onpol:  4  running.....
data buffer size:  104000
paths_onpol:  5  running.....
data buffer size:  105000
paths_onpol:  6  running.....
data buffer size:  106000
paths_onpol:  7  running.....
data buffer size:  107000
paths_onpol:  8  running.....
data buffer size:  108000
paths_onpol:  9  running.....
data buffer size:  109000
-------------------------------------
|       Iteration |               9 |
|     AverageCost |        5.65e+03 |
|         StdCost |             746 |
|     MinimumCost |        4.58e+03 |
|     MaximumCost |         6.8e+03 |
|   AverageReturn |            -414 |
|       StdReturn |            65.4 |
|   MinimumReturn |            -512 |
|   MaximumReturn |            -290 |
-------------------------------------
onpol_iters:  10
paths_onpol:  0  running.....
data buffer size:  110000
paths_onpol:  1  running.....
data buffer size:  111000
paths_onpol:  2  running.....
data buffer size:  112000
paths_onpol:  3  running.....
data buffer size:  113000
paths_onpol:  4  running.....
data buffer size:  114000
paths_onpol:  5  running.....
data buffer size:  115000
paths_onpol:  6  running.....
data buffer size:  116000
paths_onpol:  7  running.....
data buffer size:  117000
paths_onpol:  8  running.....
data buffer size:  118000
paths_onpol:  9  running.....
data buffer size:  119000
-------------------------------------
|       Iteration |              10 |
|     AverageCost |        6.65e+03 |
|         StdCost |             521 |
|     MinimumCost |        5.38e+03 |
|     MaximumCost |        7.34e+03 |
|   AverageReturn |            -570 |
|       StdReturn |            76.7 |
|   MinimumReturn |            -704 |
|   MaximumReturn |            -446 |
-------------------------------------
onpol_iters:  11
paths_onpol:  0  running.....
data buffer size:  120000
paths_onpol:  1  running.....
data buffer size:  121000
paths_onpol:  2  running.....
data buffer size:  122000
paths_onpol:  3  running.....
data buffer size:  123000
paths_onpol:  4  running.....
data buffer size:  124000
paths_onpol:  5  running.....
data buffer size:  125000
paths_onpol:  6  running.....
data buffer size:  126000
paths_onpol:  7  running.....
data buffer size:  127000
paths_onpol:  8  running.....
data buffer size:  128000
paths_onpol:  9  running.....
data buffer size:  129000
-------------------------------------
|       Iteration |              11 |
|     AverageCost |        6.49e+03 |
|         StdCost |             649 |
|     MinimumCost |         5.5e+03 |
|     MaximumCost |        7.53e+03 |
|   AverageReturn |            -613 |
|       StdReturn |              87 |
|   MinimumReturn |            -737 |
|   MaximumReturn |            -428 |
-------------------------------------
onpol_iters:  12
paths_onpol:  0  running.....
data buffer size:  130000
paths_onpol:  1  running.....
data buffer size:  131000
paths_onpol:  2  running.....
data buffer size:  132000
paths_onpol:  3  running.....
data buffer size:  133000
paths_onpol:  4  running.....
data buffer size:  134000
paths_onpol:  5  running.....
data buffer size:  135000
paths_onpol:  6  running.....
data buffer size:  136000
paths_onpol:  7  running.....
data buffer size:  137000
paths_onpol:  8  running.....
data buffer size:  138000
paths_onpol:  9  running.....
data buffer size:  139000
-------------------------------------
|       Iteration |              12 |
|     AverageCost |         5.9e+03 |
|         StdCost |             709 |
|     MinimumCost |        4.29e+03 |
|     MaximumCost |        6.97e+03 |
|   AverageReturn |            -447 |
|       StdReturn |            88.3 |
|   MinimumReturn |            -602 |
|   MaximumReturn |            -319 |
-------------------------------------
onpol_iters:  13
paths_onpol:  0  running.....
data buffer size:  140000
paths_onpol:  1  running.....
data buffer size:  141000
paths_onpol:  2  running.....
data buffer size:  142000
paths_onpol:  3  running.....
data buffer size:  143000
paths_onpol:  4  running.....
data buffer size:  144000
paths_onpol:  5  running.....
data buffer size:  145000
paths_onpol:  6  running.....
data buffer size:  146000
paths_onpol:  7  running.....
data buffer size:  147000
paths_onpol:  8  running.....
data buffer size:  148000
paths_onpol:  9  running.....
data buffer size:  149000
-------------------------------------
|       Iteration |              13 |
|     AverageCost |         6.6e+03 |
|         StdCost |             750 |
|     MinimumCost |        5.16e+03 |
|     MaximumCost |        7.75e+03 |
|   AverageReturn |            -534 |
|       StdReturn |            51.3 |
|   MinimumReturn |            -612 |
|   MaximumReturn |            -426 |
-------------------------------------
onpol_iters:  14
paths_onpol:  0  running.....
data buffer size:  150000
paths_onpol:  1  running.....
data buffer size:  151000
paths_onpol:  2  running.....
data buffer size:  152000
paths_onpol:  3  running.....
data buffer size:  153000
paths_onpol:  4  running.....
data buffer size:  154000
paths_onpol:  5  running.....
data buffer size:  155000
paths_onpol:  6  running.....
data buffer size:  156000
paths_onpol:  7  running.....
data buffer size:  157000
paths_onpol:  8  running.....
data buffer size:  158000
paths_onpol:  9  running.....
data buffer size:  159000
-------------------------------------
|       Iteration |              14 |
|     AverageCost |        6.16e+03 |
|         StdCost |             503 |
|     MinimumCost |        5.43e+03 |
|     MaximumCost |        6.94e+03 |
|   AverageReturn |            -494 |
|       StdReturn |            56.4 |
|   MinimumReturn |            -606 |
|   MaximumReturn |            -374 |
-------------------------------------
Log dir data/mb_mpc_HalfCheetah-v1_12-01-2018_02-16-37 already exists! Delete it first or use a different dir
[32;1mLogging data to data/mb_mpc_HalfCheetah-v1_12-01-2018_02-16-37/log.txt[0m
-------- env info --------
observation_space:  (20,)
action_space:  (6,)
 
collecting random data .....  
random data iter  0
random data iter  1
random data iter  2
random data iter  3
random data iter  4
Traceback (most recent call last):
  File "main.py", line 345, in <module>
    main()
  File "main.py", line 341, in main
    output_activation=None,
  File "main.py", line 164, in train
    verbose=False)
  File "main.py", line 39, in sample
    st_next, _, _, _ = env.step(at)
  File "/home/baxter/gym/gym/core.py", line 96, in step
    return self._step(action)
  File "/home/baxter/Documents/nn-dynamic-simple/cheetah_env.py", line 12, in _step
    self.do_simulation(action, self.frame_skip)
  File "/home/baxter/gym/gym/envs/mujoco/mujoco_env.py", line 98, in do_simulation
    self.model.data.ctrl = ctrl
  File "/home/baxter/anaconda2/envs/py3/lib/python3.6/site-packages/mujoco_py/mjtypes.py", line 2389, in ctrl
    val_ptr = np.array(value, dtype=np.float64).ctypes.data_as(POINTER(c_double))
  File "/home/baxter/anaconda2/envs/py3/lib/python3.6/site-packages/numpy/core/_internal.py", line 243, in __init__
    def __init__(self, array, ptr=None):
KeyboardInterrupt
Log dir data/mb_mpc_HalfCheetah-v1_12-01-2018_02-16-45 already exists! Delete it first or use a different dir
[32;1mLogging data to data/mb_mpc_HalfCheetah-v1_12-01-2018_02-16-45/log.txt[0m
-------- env info --------
observation_space:  (20,)
action_space:  (6,)
 
collecting random data .....  
random data iter  0
random data iter  1
random data iter  2
random data iter  3
random data iter  4
random data iter  5
random data iter  6
random data iter  7
random data iter  8
random data iter  9
data buffer size:  10000
onpol_iters:  0
Model fitting for  260 times ... 
loss  0  :  0.9742134
loss  1  :  0.8798088
loss  2  :  0.7609543
loss  3  :  0.7236767
loss  4  :  0.6294802
loss  5  :  0.54259145
loss  6  :  0.4660789
loss  7  :  0.39697388
loss  8  :  0.33972445
loss  9  :  0.282507
loss  10  :  0.23863754
loss  11  :  0.21746488
loss  12  :  0.21523866
loss  13  :  0.15682144
loss  14  :  0.16349368
loss  15  :  0.15156758
loss  16  :  0.15082142
loss  17  :  0.15263882
loss  18  :  0.14933968
loss  19  :  0.14470018
loss  20  :  0.13004732
loss  21  :  0.1282861
loss  22  :  0.12398426
loss  23  :  0.11394191
loss  24  :  0.10920163
loss  25  :  0.102171496
loss  26  :  0.10166913
loss  27  :  0.089500844
loss  28  :  0.097391985
loss  29  :  0.08911438
loss  30  :  0.09154546
loss  31  :  0.087760076
loss  32  :  0.09240493
loss  33  :  0.08243047
loss  34  :  0.08384928
loss  35  :  0.07684355
loss  36  :  0.077466615
loss  37  :  0.0840609
loss  38  :  0.083113
loss  39  :  0.07079313
loss  40  :  0.08286133
loss  41  :  0.081612065
loss  42  :  0.0807706
loss  43  :  0.06351006
loss  44  :  0.07926323
loss  45  :  0.07270673
loss  46  :  0.0712499
loss  47  :  0.07672774
loss  48  :  0.067628205
loss  49  :  0.06836122
loss  50  :  0.07460445
loss  51  :  0.052406095
loss  52  :  0.061235756
loss  53  :  0.060536683
loss  54  :  0.076101296
loss  55  :  0.06363632
loss  56  :  0.06489389
loss  57  :  0.06335618
loss  58  :  0.06414408
loss  59  :  0.054701727
loss  60  :  0.053165734
loss  61  :  0.057966877
loss  62  :  0.06376309
loss  63  :  0.056558263
loss  64  :  0.051323723
loss  65  :  0.049609452
loss  66  :  0.054273296
loss  67  :  0.054686762
loss  68  :  0.05162949
loss  69  :  0.05112092
loss  70  :  0.054043494
loss  71  :  0.04517262
loss  72  :  0.059527256
loss  73  :  0.049239863
loss  74  :  0.047689844
loss  75  :  0.04526023
loss  76  :  0.050310932
loss  77  :  0.05549615
loss  78  :  0.047722213
loss  79  :  0.050952464
loss  80  :  0.05667486
loss  81  :  0.051653154
loss  82  :  0.052252986
loss  83  :  0.048328493
loss  84  :  0.052852947
loss  85  :  0.0504771
loss  86  :  0.042898886
loss  87  :  0.04363361
loss  88  :  0.051062644
loss  89  :  0.04955118
loss  90  :  0.051120937
loss  91  :  0.042678274
loss  92  :  0.04910537
loss  93  :  0.041260373
loss  94  :  0.051745605
loss  95  :  0.05709965
loss  96  :  0.04878976
loss  97  :  0.044950884
loss  98  :  0.04382193
loss  99  :  0.04604665
loss  100  :  0.04285009
loss  101  :  0.050869368
loss  102  :  0.043159135
loss  103  :  0.04738609
loss  104  :  0.051155854
loss  105  :  0.042379774
loss  106  :  0.05597312
loss  107  :  0.043221712
loss  108  :  0.043321803
loss  109  :  0.049781956
loss  110  :  0.051527668
loss  111  :  0.03913346
loss  112  :  0.051827468
loss  113  :  0.036133014
loss  114  :  0.045606866
loss  115  :  0.043771047
loss  116  :  0.048012536
loss  117  :  0.038967032
loss  118  :  0.043599024
loss  119  :  0.042686064
loss  120  :  0.0440223
loss  121  :  0.04307445
loss  122  :  0.04375878
loss  123  :  0.036263056
loss  124  :  0.043841325
loss  125  :  0.04150361
loss  126  :  0.04651966
loss  127  :  0.041553177
loss  128  :  0.045692086
loss  129  :  0.041728035
loss  130  :  0.04676286
loss  131  :  0.046125352
loss  132  :  0.04280295
loss  133  :  0.042670555
loss  134  :  0.04543697
loss  135  :  0.047082312
loss  136  :  0.041406758
loss  137  :  0.04647106
loss  138  :  0.04399701
loss  139  :  0.044883765
loss  140  :  0.04044546
loss  141  :  0.038985744
loss  142  :  0.038341295
loss  143  :  0.041847516
loss  144  :  0.038686402
loss  145  :  0.042095017
loss  146  :  0.038894743
loss  147  :  0.03556258
loss  148  :  0.036953345
loss  149  :  0.037387148
loss  150  :  0.039961837
loss  151  :  0.04118686
loss  152  :  0.041593045
loss  153  :  0.03886441
loss  154  :  0.039695106
loss  155  :  0.040391825
loss  156  :  0.036974493
loss  157  :  0.042827196
loss  158  :  0.039596103
loss  159  :  0.0338558
loss  160  :  0.031818293
loss  161  :  0.043863904
loss  162  :  0.03969939
loss  163  :  0.038694408
loss  164  :  0.032549463
loss  165  :  0.032044385
loss  166  :  0.04470726
loss  167  :  0.037272178
loss  168  :  0.033511676
loss  169  :  0.038226485
loss  170  :  0.037537955
loss  171  :  0.033228
loss  172  :  0.038485818
loss  173  :  0.042779602
loss  174  :  0.040758837
loss  175  :  0.042545803
loss  176  :  0.038931113
loss  177  :  0.03655048
loss  178  :  0.03426322
loss  179  :  0.040577162
loss  180  :  0.03264136
loss  181  :  0.03268661
loss  182  :  0.04020082
loss  183  :  0.03236047
loss  184  :  0.038369752
loss  185  :  0.029768694
loss  186  :  0.03337707
loss  187  :  0.040467955
loss  188  :  0.037916757
loss  189  :  0.03639128
loss  190  :  0.03317972
loss  191  :  0.033275127
loss  192  :  0.035496265
loss  193  :  0.0408799
loss  194  :  0.04327254
loss  195  :  0.032140546
loss  196  :  0.036858175
loss  197  :  0.038547643
loss  198  :  0.03424809
loss  199  :  0.032149255
loss  200  :  0.03649355
loss  201  :  0.03677552
loss  202  :  0.032418866
loss  203  :  0.035458136
loss  204  :  0.033300553
loss  205  :  0.03183639
loss  206  :  0.033755276
loss  207  :  0.031751804
loss  208  :  0.03346876
loss  209  :  0.03144595
loss  210  :  0.03175661
loss  211  :  0.031524766
loss  212  :  0.036613524
loss  213  :  0.028463144
loss  214  :  0.034507476
loss  215  :  0.027632225
loss  216  :  0.027259996
loss  217  :  0.03434161
loss  218  :  0.030806297
loss  219  :  0.031098729
loss  220  :  0.03333599
loss  221  :  0.03559904
loss  222  :  0.031984158
loss  223  :  0.03026932
loss  224  :  0.030796343
loss  225  :  0.03210657
loss  226  :  0.031956296
loss  227  :  0.030767197
loss  228  :  0.030533439
loss  229  :  0.037679307
loss  230  :  0.028885972
loss  231  :  0.030533548
loss  232  :  0.031640448
loss  233  :  0.03232031
loss  234  :  0.029004639
loss  235  :  0.029491812
loss  236  :  0.030341322
loss  237  :  0.0294741
loss  238  :  0.029657006
loss  239  :  0.033195958
loss  240  :  0.028548772
loss  241  :  0.03312836
loss  242  :  0.03373763
loss  243  :  0.032177158
loss  244  :  0.03518527
loss  245  :  0.026993776
loss  246  :  0.03236365
loss  247  :  0.02991639
loss  248  :  0.031600505
loss  249  :  0.027904773
loss  250  :  0.028681362
loss  251  :  0.0336532
loss  252  :  0.0291794
loss  253  :  0.03324243
loss  254  :  0.027076775
loss  255  :  0.02704179
loss  256  :  0.028133338
loss  257  :  0.025506457
loss  258  :  0.022831108
loss  259  :  0.027028795
paths_onpol:  0  running.....
data buffer size:  10000
total return:  33.26488324238662
costs:  375.3216669397714
paths_onpol:  1  running.....
data buffer size:  11000
total return:  92.85639787537512
costs:  117.16621737280563
paths_onpol:  2  running.....
data buffer size:  12000
total return:  118.49014040736964
costs:  -127.05695092801211
paths_onpol:  3  running.....
data buffer size:  13000
total return:  115.15319824347914
costs:  -75.59881384937883
paths_onpol:  4  running.....
data buffer size:  14000
total return:  107.97605654414357
costs:  63.04911620007812
paths_onpol:  5  running.....
data buffer size:  15000
total return:  134.79324779274285
costs:  107.76987368250524
paths_onpol:  6  running.....
data buffer size:  16000
total return:  67.8727216952404
costs:  156.05941232217026
paths_onpol:  7  running.....
data buffer size:  17000
total return:  102.03696005096185
costs:  135.79848091823362
paths_onpol:  8  running.....
data buffer size:  18000
total return:  95.54324210660036
costs:  55.57497210356924
paths_onpol:  9  running.....
data buffer size:  19000
total return:  37.0533005939532
costs:  140.18553485326146
-------------------------------------
|       Iteration |               0 |
|     AverageCost |            94.8 |
|         StdCost |             129 |
|     MinimumCost |            -127 |
|     MaximumCost |             375 |
|   AverageReturn |            90.5 |
|       StdReturn |            32.4 |
|   MinimumReturn |            33.3 |
|   MaximumReturn |             135 |
-------------------------------------
onpol_iters:  1
Model fitting for  260 times ... 
loss  0  :  0.057457816
loss  1  :  0.060108602
loss  2  :  0.041267853
loss  3  :  0.056015838
loss  4  :  0.04477896
loss  5  :  0.04901879
loss  6  :  0.0429414
loss  7  :  0.045880668
loss  8  :  0.053136796
loss  9  :  0.042432345
loss  10  :  0.045262523
loss  11  :  0.04049142
loss  12  :  0.04628508
loss  13  :  0.03417975
loss  14  :  0.037874866
loss  15  :  0.043620847
loss  16  :  0.036637753
loss  17  :  0.04108465
loss  18  :  0.037781112
loss  19  :  0.040389154
loss  20  :  0.036636136
loss  21  :  0.04318122
loss  22  :  0.0463928
loss  23  :  0.041228432
loss  24  :  0.04154452
loss  25  :  0.03667941
loss  26  :  0.03315464
loss  27  :  0.039199375
loss  28  :  0.030281186
loss  29  :  0.030847633
loss  30  :  0.029865582
loss  31  :  0.042865418
loss  32  :  0.03381311
loss  33  :  0.037281863
loss  34  :  0.029271102
loss  35  :  0.037658576
loss  36  :  0.034780513
loss  37  :  0.03318095
loss  38  :  0.03546696
loss  39  :  0.030370671
loss  40  :  0.03531197
loss  41  :  0.030857662
loss  42  :  0.028395226
loss  43  :  0.030198216
loss  44  :  0.030255288
loss  45  :  0.028547725
loss  46  :  0.039493017
loss  47  :  0.032485973
loss  48  :  0.03350978
loss  49  :  0.029904764
loss  50  :  0.033189416
loss  51  :  0.032791324
loss  52  :  0.031583697
loss  53  :  0.030100157
loss  54  :  0.03351094
loss  55  :  0.028804015
loss  56  :  0.029655436
loss  57  :  0.034695704
loss  58  :  0.030658102
loss  59  :  0.029394466
loss  60  :  0.026967824
loss  61  :  0.029731203
loss  62  :  0.024090016
loss  63  :  0.027268792
loss  64  :  0.030719995
loss  65  :  0.032182477
loss  66  :  0.027890336
loss  67  :  0.027929086
loss  68  :  0.031775285
loss  69  :  0.031388503
loss  70  :  0.031241715
loss  71  :  0.028739786
loss  72  :  0.026956996
loss  73  :  0.024986483
loss  74  :  0.025045782
loss  75  :  0.028760498
loss  76  :  0.03339984
loss  77  :  0.028240448
loss  78  :  0.024779234
loss  79  :  0.026242912
loss  80  :  0.024097437
loss  81  :  0.02966795
loss  82  :  0.023569927
loss  83  :  0.032563705
loss  84  :  0.028723124
loss  85  :  0.0287617
loss  86  :  0.027124697
loss  87  :  0.0241835
loss  88  :  0.023908833
loss  89  :  0.023606565
loss  90  :  0.026926685
loss  91  :  0.02613366
loss  92  :  0.023894727
loss  93  :  0.025497073
loss  94  :  0.023486663
loss  95  :  0.028988246
loss  96  :  0.027676806
loss  97  :  0.026960116
loss  98  :  0.023687938
loss  99  :  0.025395941
loss  100  :  0.02699793
loss  101  :  0.029649299
loss  102  :  0.027633388
loss  103  :  0.025679458
loss  104  :  0.026616048
loss  105  :  0.024759667
loss  106  :  0.02049346
loss  107  :  0.025578987
loss  108  :  0.025572991
loss  109  :  0.026485777
loss  110  :  0.026248747
loss  111  :  0.022140156
loss  112  :  0.026614737
loss  113  :  0.022741213
loss  114  :  0.022424942
loss  115  :  0.030553639
loss  116  :  0.02289806
loss  117  :  0.027517522
loss  118  :  0.022970593
loss  119  :  0.028964326
loss  120  :  0.021549217
loss  121  :  0.023766737
loss  122  :  0.030704543
loss  123  :  0.02757143
loss  124  :  0.024361735
loss  125  :  0.027523011
loss  126  :  0.025208313
loss  127  :  0.025472444
loss  128  :  0.024184464
loss  129  :  0.025486832
loss  130  :  0.022998812
loss  131  :  0.024346352
loss  132  :  0.026598256
loss  133  :  0.025841545
loss  134  :  0.022897804
loss  135  :  0.02479149
loss  136  :  0.019384038
loss  137  :  0.027038177
loss  138  :  0.021477988
loss  139  :  0.022883806
loss  140  :  0.026439656
loss  141  :  0.025083605
loss  142  :  0.02123239
loss  143  :  0.022021567
loss  144  :  0.023839949
loss  145  :  0.022808138
loss  146  :  0.024331974
loss  147  :  0.020124067
loss  148  :  0.022736868
loss  149  :  0.021448188
loss  150  :  0.023745244
loss  151  :  0.019767538
loss  152  :  0.022593841
loss  153  :  0.025516093
loss  154  :  0.022340352
loss  155  :  0.02511682
loss  156  :  0.02599911
loss  157  :  0.020351384
loss  158  :  0.02119582
loss  159  :  0.024443453
loss  160  :  0.024706297
loss  161  :  0.024719128
loss  162  :  0.022267774
loss  163  :  0.01921086
loss  164  :  0.023420984
loss  165  :  0.025794348
loss  166  :  0.025662486
loss  167  :  0.021670857
loss  168  :  0.022418728
loss  169  :  0.022758942
loss  170  :  0.02126198
loss  171  :  0.021210881
loss  172  :  0.021380495
loss  173  :  0.021064632
loss  174  :  0.022944877
loss  175  :  0.021277567
loss  176  :  0.021972816
loss  177  :  0.022204163
loss  178  :  0.018831106
loss  179  :  0.019194787
loss  180  :  0.018788986
loss  181  :  0.018236008
loss  182  :  0.026012719
loss  183  :  0.018992804
loss  184  :  0.019741528
loss  185  :  0.02230757
loss  186  :  0.019928163
loss  187  :  0.021155158
loss  188  :  0.028361613
loss  189  :  0.01990354
loss  190  :  0.028274346
loss  191  :  0.020922398
loss  192  :  0.021217879
loss  193  :  0.019628394
loss  194  :  0.021380864
loss  195  :  0.018122025
loss  196  :  0.021618417
loss  197  :  0.01845052
loss  198  :  0.021627024
loss  199  :  0.019995313
loss  200  :  0.02169612
loss  201  :  0.021390477
loss  202  :  0.021684531
loss  203  :  0.023573548
loss  204  :  0.019750813
loss  205  :  0.018826544
loss  206  :  0.020035688
loss  207  :  0.018567182
loss  208  :  0.023962503
loss  209  :  0.019109707
loss  210  :  0.020422116
loss  211  :  0.01986624
loss  212  :  0.021193685
loss  213  :  0.017613405
loss  214  :  0.023484841
loss  215  :  0.017701214
loss  216  :  0.015874399
loss  217  :  0.023080725
loss  218  :  0.01844286
loss  219  :  0.02383976
loss  220  :  0.019770265
loss  221  :  0.02099082
loss  222  :  0.019945906
loss  223  :  0.018715974
loss  224  :  0.020612186
loss  225  :  0.020109821
loss  226  :  0.019778946
loss  227  :  0.021238446
loss  228  :  0.018615609
loss  229  :  0.020402562
loss  230  :  0.024259666
loss  231  :  0.021436179
loss  232  :  0.018677264
loss  233  :  0.021814499
loss  234  :  0.020245582
loss  235  :  0.020314336
loss  236  :  0.016747672
loss  237  :  0.017613517
loss  238  :  0.018587997
loss  239  :  0.018512145
loss  240  :  0.020339448
loss  241  :  0.017261926
loss  242  :  0.018540654
loss  243  :  0.020397747
loss  244  :  0.01997542
loss  245  :  0.018480409
loss  246  :  0.01791485
loss  247  :  0.018828627
loss  248  :  0.02454612
loss  249  :  0.016464569
loss  250  :  0.018028757
loss  251  :  0.016186258
loss  252  :  0.01651518
loss  253  :  0.0165635
loss  254  :  0.02095682
loss  255  :  0.019635603
loss  256  :  0.015232086
loss  257  :  0.01851747
loss  258  :  0.01991346
loss  259  :  0.01909364
paths_onpol:  0  running.....
data buffer size:  20000
total return:  335.23477221252176
costs:  -401.3589246964428
paths_onpol:  1  running.....
data buffer size:  21000
total return:  361.67330938852837
costs:  -336.5617097146185
paths_onpol:  2  running.....
data buffer size:  22000
total return:  315.646871253275
costs:  -293.66095249862207
paths_onpol:  3  running.....
data buffer size:  23000
total return:  374.99887292724657
costs:  -352.1142702070006
paths_onpol:  4  running.....
data buffer size:  24000
total return:  287.3758404741786
costs:  -239.7596356744183
paths_onpol:  5  running.....
data buffer size:  25000
total return:  304.32564971586595
costs:  -315.05673923164085
paths_onpol:  6  running.....
data buffer size:  26000
total return:  311.71666411053735
costs:  -276.84904761848236
paths_onpol:  7  running.....
data buffer size:  27000
total return:  306.79627917671223
costs:  -352.5956125988796
paths_onpol:  8  running.....
data buffer size:  28000
total return:  315.45232973587986
costs:  -380.58671304851333
paths_onpol:  9  running.....
data buffer size:  29000
total return:  318.6372125274395
costs:  -292.8675066084455
-------------------------------------
|       Iteration |               1 |
|     AverageCost |            -324 |
|         StdCost |            47.2 |
|     MinimumCost |            -401 |
|     MaximumCost |            -240 |
|   AverageReturn |             323 |
|       StdReturn |            25.5 |
|   MinimumReturn |             287 |
|   MaximumReturn |             375 |
-------------------------------------
onpol_iters:  2
Model fitting for  260 times ... 
loss  0  :  0.022104675
loss  1  :  0.020017345
loss  2  :  0.021973496
loss  3  :  0.021662358
loss  4  :  0.023671698
loss  5  :  0.023745283
loss  6  :  0.02069011
loss  7  :  0.019177148
loss  8  :  0.01811828
loss  9  :  0.019041315
loss  10  :  0.023024613
loss  11  :  0.020084243
loss  12  :  0.018319458
loss  13  :  0.018649917
loss  14  :  0.019144699
loss  15  :  0.020187806
loss  16  :  0.022725161
loss  17  :  0.021919385
loss  18  :  0.025651813
loss  19  :  0.018806014
loss  20  :  0.018060397
loss  21  :  0.01825125
loss  22  :  0.018524434
loss  23  :  0.020532262
loss  24  :  0.015618051
loss  25  :  0.016481886
loss  26  :  0.019549755
loss  27  :  0.018613085
loss  28  :  0.022154473
loss  29  :  0.02136817
loss  30  :  0.020641897
loss  31  :  0.018689325
loss  32  :  0.021196047
loss  33  :  0.020969983
loss  34  :  0.016602471
loss  35  :  0.02110199
loss  36  :  0.019868214
loss  37  :  0.023287782
loss  38  :  0.020868324
loss  39  :  0.026044656
loss  40  :  0.018808046
loss  41  :  0.022347327
loss  42  :  0.017045494
loss  43  :  0.020489637
loss  44  :  0.018940762
loss  45  :  0.021447528
loss  46  :  0.019064728
loss  47  :  0.019542733
loss  48  :  0.01981983
loss  49  :  0.017972091
loss  50  :  0.01896896
loss  51  :  0.01803429
loss  52  :  0.01965323
loss  53  :  0.016001299
loss  54  :  0.018510936
loss  55  :  0.015261397
loss  56  :  0.016885338
loss  57  :  0.017307436
loss  58  :  0.01914063
loss  59  :  0.016868405
loss  60  :  0.017255899
loss  61  :  0.021423649
loss  62  :  0.02099275
loss  63  :  0.02162288
loss  64  :  0.016511723
loss  65  :  0.018919706
loss  66  :  0.019741673
loss  67  :  0.014909573
loss  68  :  0.0177049
loss  69  :  0.021194745
loss  70  :  0.01769049
loss  71  :  0.014978158
loss  72  :  0.016227175
loss  73  :  0.016932126
loss  74  :  0.018795684
loss  75  :  0.017234515
loss  76  :  0.01586091
loss  77  :  0.0160782
loss  78  :  0.015968021
loss  79  :  0.016008694
loss  80  :  0.019245278
loss  81  :  0.019883234
loss  82  :  0.014561484
loss  83  :  0.014488635
loss  84  :  0.016273748
loss  85  :  0.020034123
loss  86  :  0.014863683
loss  87  :  0.016235318
loss  88  :  0.016749319
loss  89  :  0.017755643
loss  90  :  0.016739478
loss  91  :  0.019788157
loss  92  :  0.019325135
loss  93  :  0.018365739
loss  94  :  0.01630814
loss  95  :  0.017888684
loss  96  :  0.0138705615
loss  97  :  0.016416868
loss  98  :  0.01683183
loss  99  :  0.014845218
loss  100  :  0.01632163
loss  101  :  0.019975703
loss  102  :  0.01685365
loss  103  :  0.014824772
loss  104  :  0.015222689
loss  105  :  0.015662327
loss  106  :  0.01445177
loss  107  :  0.013137631
loss  108  :  0.019598309
loss  109  :  0.02027301
loss  110  :  0.01651469
loss  111  :  0.018904
loss  112  :  0.017065842
loss  113  :  0.019266983
loss  114  :  0.0149858
loss  115  :  0.015630089
loss  116  :  0.021059362
loss  117  :  0.01435076
loss  118  :  0.01823921
loss  119  :  0.017399155
loss  120  :  0.01894999
loss  121  :  0.017411526
loss  122  :  0.019564146
loss  123  :  0.01462768
loss  124  :  0.017624695
loss  125  :  0.017050898
loss  126  :  0.01536787
loss  127  :  0.016151814
loss  128  :  0.018011454
loss  129  :  0.01641886
loss  130  :  0.01824623
loss  131  :  0.017457038
loss  132  :  0.01882786
loss  133  :  0.0149629805
loss  134  :  0.015720049
loss  135  :  0.017286241
loss  136  :  0.016788715
loss  137  :  0.013280016
loss  138  :  0.014776421
loss  139  :  0.016075686
loss  140  :  0.018079497
loss  141  :  0.015719563
loss  142  :  0.013480991
loss  143  :  0.0155522
loss  144  :  0.018053044
loss  145  :  0.014439884
loss  146  :  0.015477533
loss  147  :  0.016585942
loss  148  :  0.013593215
loss  149  :  0.01463259
loss  150  :  0.014640704
loss  151  :  0.01504973
loss  152  :  0.013893487
loss  153  :  0.015152456
loss  154  :  0.015627246
loss  155  :  0.013664724
loss  156  :  0.0146848755
loss  157  :  0.013108822
loss  158  :  0.014083544
loss  159  :  0.013695346
loss  160  :  0.012282761
loss  161  :  0.013067749
loss  162  :  0.01796966
loss  163  :  0.015312642
loss  164  :  0.014042467
loss  165  :  0.014367002
loss  166  :  0.013318469
loss  167  :  0.01382675
loss  168  :  0.015122068
loss  169  :  0.012631716
loss  170  :  0.012822608
loss  171  :  0.012551537
loss  172  :  0.0150242895
loss  173  :  0.01612078
loss  174  :  0.014937624
loss  175  :  0.015035148
loss  176  :  0.01291616
loss  177  :  0.01621027
loss  178  :  0.022535905
loss  179  :  0.013243404
loss  180  :  0.012794423
loss  181  :  0.012545806
loss  182  :  0.016083367
loss  183  :  0.01489189
loss  184  :  0.016204545
loss  185  :  0.014528464
loss  186  :  0.014263956
loss  187  :  0.014915648
loss  188  :  0.01275927
loss  189  :  0.013959912
loss  190  :  0.015034467
loss  191  :  0.014005372
loss  192  :  0.016040519
loss  193  :  0.015306774
loss  194  :  0.0163791
loss  195  :  0.015504135
loss  196  :  0.017028889
loss  197  :  0.016457245
loss  198  :  0.015077164
loss  199  :  0.015946206
loss  200  :  0.014034283
loss  201  :  0.015914064
loss  202  :  0.013035104
loss  203  :  0.014717719
loss  204  :  0.014534375
loss  205  :  0.015690813
loss  206  :  0.013697172
loss  207  :  0.012996199
loss  208  :  0.012984784
loss  209  :  0.01263383
loss  210  :  0.013657307
loss  211  :  0.012907699
loss  212  :  0.016216869
loss  213  :  0.014493428
loss  214  :  0.013701342
loss  215  :  0.0135822715
loss  216  :  0.0126553895
loss  217  :  0.014824418
loss  218  :  0.014815604
loss  219  :  0.016110864
loss  220  :  0.015383968
loss  221  :  0.0153234275
loss  222  :  0.011321785
loss  223  :  0.012162289
loss  224  :  0.012880063
loss  225  :  0.014030752
loss  226  :  0.011071511
loss  227  :  0.012791182
loss  228  :  0.014959291
loss  229  :  0.012842497
loss  230  :  0.0156718
loss  231  :  0.011497745
loss  232  :  0.012310652
loss  233  :  0.014252218
loss  234  :  0.014230594
loss  235  :  0.014369729
loss  236  :  0.016920328
loss  237  :  0.013735418
loss  238  :  0.014587286
loss  239  :  0.01653885
loss  240  :  0.016582623
loss  241  :  0.014605914
loss  242  :  0.012525359
loss  243  :  0.015291266
loss  244  :  0.012996522
loss  245  :  0.012270378
loss  246  :  0.014296517
loss  247  :  0.01305365
loss  248  :  0.014300111
loss  249  :  0.016133705
loss  250  :  0.013658551
loss  251  :  0.0121105295
loss  252  :  0.014697756
loss  253  :  0.014192844
loss  254  :  0.013069962
loss  255  :  0.012994802
loss  256  :  0.012512157
loss  257  :  0.015186304
loss  258  :  0.012005761
loss  259  :  0.015045834
paths_onpol:  0  running.....
data buffer size:  30000
total return:  721.199236776054
costs:  -893.1347838008577
paths_onpol:  1  running.....
data buffer size:  31000
total return:  778.2673765694049
costs:  -816.5805755590835
paths_onpol:  2  running.....
data buffer size:  32000
total return:  785.2847771197538
costs:  -939.3518270090366
paths_onpol:  3  running.....
data buffer size:  33000
total return:  654.4972780394837
costs:  -771.4796654533103
paths_onpol:  4  running.....
data buffer size:  34000
total return:  753.0072025544908
costs:  -835.6338666426412
paths_onpol:  5  running.....
data buffer size:  35000
total return:  788.5764935183346
costs:  -882.8492087364444
paths_onpol:  6  running.....
data buffer size:  36000
total return:  807.9438274727364
costs:  -895.2788191217661
paths_onpol:  7  running.....
data buffer size:  37000
total return:  709.3965409564771
costs:  -793.2115149823552
paths_onpol:  8  running.....
data buffer size:  38000
total return:  778.9752269845055
costs:  -817.8361647039438
paths_onpol:  9  running.....
data buffer size:  39000
total return:  704.9091015159344
costs:  -779.8163119332477
-------------------------------------
|       Iteration |               2 |
|     AverageCost |            -843 |
|         StdCost |            53.9 |
|     MinimumCost |            -939 |
|     MaximumCost |            -771 |
|   AverageReturn |             748 |
|       StdReturn |            46.2 |
|   MinimumReturn |             654 |
|   MaximumReturn |             808 |
-------------------------------------
onpol_iters:  3
Model fitting for  260 times ... 
loss  0  :  0.020321283
loss  1  :  0.016804582
loss  2  :  0.013596823
loss  3  :  0.015804201
loss  4  :  0.014525866
loss  5  :  0.017269451
loss  6  :  0.015606269
loss  7  :  0.015382519
loss  8  :  0.017159704
loss  9  :  0.01661131
loss  10  :  0.01905317
loss  11  :  0.017637888
loss  12  :  0.01344884
loss  13  :  0.015209856
loss  14  :  0.015217441
loss  15  :  0.016993817
loss  16  :  0.014628393
loss  17  :  0.015819034
loss  18  :  0.017200697
loss  19  :  0.015678618
loss  20  :  0.02002794
loss  21  :  0.015042973
loss  22  :  0.015359697
loss  23  :  0.018675815
loss  24  :  0.014155408
loss  25  :  0.022909008
loss  26  :  0.015534329
loss  27  :  0.016025474
loss  28  :  0.015283227
loss  29  :  0.017017733
loss  30  :  0.015704418
loss  31  :  0.01724722
loss  32  :  0.016165704
loss  33  :  0.015940595
loss  34  :  0.015559751
loss  35  :  0.014685494
loss  36  :  0.015795594
loss  37  :  0.015802037
loss  38  :  0.015607444
loss  39  :  0.016151832
loss  40  :  0.013449142
loss  41  :  0.014984669
loss  42  :  0.014300758
loss  43  :  0.016181834
loss  44  :  0.016422784
loss  45  :  0.020488227
loss  46  :  0.01634841
loss  47  :  0.019844143
loss  48  :  0.01859906
loss  49  :  0.016751
loss  50  :  0.015433023
loss  51  :  0.01793978
loss  52  :  0.016021669
loss  53  :  0.013539213
loss  54  :  0.017256415
loss  55  :  0.017537665
loss  56  :  0.017807636
loss  57  :  0.017065067
loss  58  :  0.013978672
loss  59  :  0.013546586
loss  60  :  0.016501602
loss  61  :  0.017303519
loss  62  :  0.015198542
loss  63  :  0.016823512
loss  64  :  0.013119811
loss  65  :  0.014706215
loss  66  :  0.014939967
loss  67  :  0.019629816
loss  68  :  0.01736609
loss  69  :  0.013825434
loss  70  :  0.015744884
loss  71  :  0.012929231
loss  72  :  0.017048648
loss  73  :  0.012665316
loss  74  :  0.014455527
loss  75  :  0.013301534
loss  76  :  0.016370466
loss  77  :  0.013243777
loss  78  :  0.015250698
loss  79  :  0.014337599
loss  80  :  0.0117946
loss  81  :  0.016815888
loss  82  :  0.015052739
loss  83  :  0.015556159
loss  84  :  0.017610338
loss  85  :  0.013428834
loss  86  :  0.014599338
loss  87  :  0.014052844
loss  88  :  0.016676089
loss  89  :  0.012795526
loss  90  :  0.014549372
loss  91  :  0.016826201
loss  92  :  0.012405702
loss  93  :  0.01708332
loss  94  :  0.014223309
loss  95  :  0.014020053
loss  96  :  0.01368503
loss  97  :  0.015757043
loss  98  :  0.014973092
loss  99  :  0.0133317355
loss  100  :  0.012647077
loss  101  :  0.013973175
loss  102  :  0.013521153
loss  103  :  0.01321443
loss  104  :  0.015863866
loss  105  :  0.012467573
loss  106  :  0.014128484
loss  107  :  0.0128439395
loss  108  :  0.012792157
loss  109  :  0.012559396
loss  110  :  0.013917593
loss  111  :  0.015424082
loss  112  :  0.013730126
loss  113  :  0.014059449
loss  114  :  0.013702674
loss  115  :  0.0129547
loss  116  :  0.014935556
loss  117  :  0.013698469
loss  118  :  0.012320047
loss  119  :  0.015314194
loss  120  :  0.015376816
loss  121  :  0.012086039
loss  122  :  0.010878842
loss  123  :  0.013567634
loss  124  :  0.015456664
loss  125  :  0.010529464
loss  126  :  0.015215926
loss  127  :  0.013798405
loss  128  :  0.021407347
loss  129  :  0.013343355
loss  130  :  0.014651063
loss  131  :  0.014052028
loss  132  :  0.0124574425
loss  133  :  0.012971771
loss  134  :  0.012609551
loss  135  :  0.013224867
loss  136  :  0.015732942
loss  137  :  0.011984832
loss  138  :  0.0141840335
loss  139  :  0.013289705
loss  140  :  0.012427522
loss  141  :  0.01381926
loss  142  :  0.011206555
loss  143  :  0.015060678
loss  144  :  0.014410269
loss  145  :  0.014448029
loss  146  :  0.012816389
loss  147  :  0.011774765
loss  148  :  0.012621125
loss  149  :  0.0118914405
loss  150  :  0.011804854
loss  151  :  0.014017972
loss  152  :  0.01389693
loss  153  :  0.014007479
loss  154  :  0.011556327
loss  155  :  0.013189433
loss  156  :  0.012169109
loss  157  :  0.013471624
loss  158  :  0.011834416
loss  159  :  0.013893175
loss  160  :  0.012962064
loss  161  :  0.01261172
loss  162  :  0.013018307
loss  163  :  0.012391096
loss  164  :  0.014164087
loss  165  :  0.014837375
loss  166  :  0.014186939
loss  167  :  0.014242241
loss  168  :  0.011963574
loss  169  :  0.013195328
loss  170  :  0.013934013
loss  171  :  0.012663402
loss  172  :  0.014121661
loss  173  :  0.014403189
loss  174  :  0.012674066
loss  175  :  0.013295104
loss  176  :  0.01357324
loss  177  :  0.013704005
loss  178  :  0.01222945
loss  179  :  0.014259947
loss  180  :  0.014654112
loss  181  :  0.012125711
loss  182  :  0.011632584
loss  183  :  0.012691746
loss  184  :  0.0111068245
loss  185  :  0.015309915
loss  186  :  0.010135725
loss  187  :  0.013257539
loss  188  :  0.013296714
loss  189  :  0.011202495
loss  190  :  0.013693601
loss  191  :  0.012687126
loss  192  :  0.010126372
loss  193  :  0.011639418
loss  194  :  0.01242
loss  195  :  0.013750513
loss  196  :  0.014283687
loss  197  :  0.016058747
loss  198  :  0.011569102
loss  199  :  0.013749929
loss  200  :  0.016402686
loss  201  :  0.014999938
loss  202  :  0.014510785
loss  203  :  0.016136672
loss  204  :  0.014829703
loss  205  :  0.012955753
loss  206  :  0.010046231
loss  207  :  0.013422778
loss  208  :  0.011374878
loss  209  :  0.012915039
loss  210  :  0.013839188
loss  211  :  0.013579138
loss  212  :  0.010326206
loss  213  :  0.011640612
loss  214  :  0.013186957
loss  215  :  0.012219036
loss  216  :  0.013050583
loss  217  :  0.0132341655
loss  218  :  0.011255656
loss  219  :  0.013540536
loss  220  :  0.010613042
loss  221  :  0.01234245
loss  222  :  0.013152985
loss  223  :  0.012247004
loss  224  :  0.01150953
loss  225  :  0.012851948
loss  226  :  0.011011537
loss  227  :  0.01469399
loss  228  :  0.014262644
loss  229  :  0.0142415045
loss  230  :  0.013194406
loss  231  :  0.012509016
loss  232  :  0.011369738
loss  233  :  0.012295766
loss  234  :  0.010879886
loss  235  :  0.013658049
loss  236  :  0.012855882
loss  237  :  0.013090545
loss  238  :  0.014673904
loss  239  :  0.011301193
loss  240  :  0.011712661
loss  241  :  0.012219968
loss  242  :  0.01159129
loss  243  :  0.01290887
loss  244  :  0.011119971
loss  245  :  0.011720416
loss  246  :  0.014087458
loss  247  :  0.011374617
loss  248  :  0.013123045
loss  249  :  0.012180668
loss  250  :  0.011708012
loss  251  :  0.014333218
loss  252  :  0.01296097
loss  253  :  0.012961702
loss  254  :  0.01237618
loss  255  :  0.011405973
loss  256  :  0.013747084
loss  257  :  0.015250923
loss  258  :  0.012200141
loss  259  :  0.013092649
paths_onpol:  0  running.....
data buffer size:  40000
total return:  900.8105350852136
costs:  -1069.3192934021126
paths_onpol:  1  running.....
data buffer size:  41000
total return:  973.3389625377077
costs:  -1047.446287810344
paths_onpol:  2  running.....
data buffer size:  42000
total return:  947.2997094204852
costs:  -1070.4961704841069
paths_onpol:  3  running.....
data buffer size:  43000
total return:  922.2115468164237
costs:  -1075.1396780577352
paths_onpol:  4  running.....
data buffer size:  44000
total return:  857.8555215997243
costs:  -1010.2502015135296
paths_onpol:  5  running.....
data buffer size:  45000
total return:  929.1498868682481
costs:  -1044.6355817450856
paths_onpol:  6  running.....
data buffer size:  46000
total return:  946.6765294416773
costs:  -1080.5439151824485
paths_onpol:  7  running.....
data buffer size:  47000
total return:  952.1254651801281
costs:  -1043.8768570383395
paths_onpol:  8  running.....
data buffer size:  48000
total return:  934.6855248674914
costs:  -1027.0855152728832
paths_onpol:  9  running.....
data buffer size:  49000
total return:  940.7406925503516
costs:  -1070.7839573100723
-------------------------------------
|       Iteration |               3 |
|     AverageCost |       -1.05e+03 |
|         StdCost |            21.9 |
|     MinimumCost |       -1.08e+03 |
|     MaximumCost |       -1.01e+03 |
|   AverageReturn |             930 |
|       StdReturn |            30.3 |
|   MinimumReturn |             858 |
|   MaximumReturn |             973 |
-------------------------------------
onpol_iters:  4
Model fitting for  260 times ... 
loss  0  :  0.013355347
loss  1  :  0.01507144
loss  2  :  0.016363598
loss  3  :  0.014240864
loss  4  :  0.01642831
loss  5  :  0.015683513
loss  6  :  0.012574847
loss  7  :  0.022359138
loss  8  :  0.015450041
loss  9  :  0.011947205
loss  10  :  0.017549638
loss  11  :  0.019694205
loss  12  :  0.01438721
loss  13  :  0.020708648
loss  14  :  0.017654773
loss  15  :  0.013458798
loss  16  :  0.016995762
loss  17  :  0.014864218
loss  18  :  0.015144172
loss  19  :  0.015198657
loss  20  :  0.014730671
loss  21  :  0.017784419
loss  22  :  0.01641109
loss  23  :  0.013414381
loss  24  :  0.014387324
loss  25  :  0.014653413
loss  26  :  0.017845297
loss  27  :  0.014710419
loss  28  :  0.014462663
loss  29  :  0.014171059
loss  30  :  0.014395237
loss  31  :  0.018546095
loss  32  :  0.014927929
loss  33  :  0.016556744
loss  34  :  0.01499704
loss  35  :  0.014706587
loss  36  :  0.019682746
loss  37  :  0.013826114
loss  38  :  0.014865905
loss  39  :  0.012610073
loss  40  :  0.014741113
loss  41  :  0.018866872
loss  42  :  0.014732683
loss  43  :  0.015529802
loss  44  :  0.015330607
loss  45  :  0.016161934
loss  46  :  0.013714677
loss  47  :  0.013724682
loss  48  :  0.014849499
loss  49  :  0.014828776
loss  50  :  0.017996643
loss  51  :  0.016806941
loss  52  :  0.015921231
loss  53  :  0.013322356
loss  54  :  0.013344323
loss  55  :  0.012576958
loss  56  :  0.014102166
loss  57  :  0.015169645
loss  58  :  0.013860059
loss  59  :  0.012598902
loss  60  :  0.014333774
loss  61  :  0.014765995
loss  62  :  0.01462782
loss  63  :  0.014780392
loss  64  :  0.017136773
loss  65  :  0.012750504
loss  66  :  0.018041238
loss  67  :  0.012348893
loss  68  :  0.015338374
loss  69  :  0.014093024
loss  70  :  0.011327796
loss  71  :  0.018356944
loss  72  :  0.015893936
loss  73  :  0.015050679
loss  74  :  0.014476744
loss  75  :  0.016324867
loss  76  :  0.013382411
loss  77  :  0.013130176
loss  78  :  0.014971298
loss  79  :  0.013976696
loss  80  :  0.015716337
loss  81  :  0.014752897
loss  82  :  0.011211245
loss  83  :  0.01186577
loss  84  :  0.011350145
loss  85  :  0.012784017
loss  86  :  0.01194796
loss  87  :  0.012076703
loss  88  :  0.013704047
loss  89  :  0.014319122
loss  90  :  0.014143476
loss  91  :  0.013412053
loss  92  :  0.013224664
loss  93  :  0.014115989
loss  94  :  0.01180012
loss  95  :  0.013804099
loss  96  :  0.011446328
loss  97  :  0.011041991
loss  98  :  0.011491315
loss  99  :  0.012491077
loss  100  :  0.01211231
loss  101  :  0.011571199
loss  102  :  0.013398694
loss  103  :  0.01688754
loss  104  :  0.0118359625
loss  105  :  0.015490589
loss  106  :  0.012556596
loss  107  :  0.013336495
loss  108  :  0.012114137
loss  109  :  0.012381831
loss  110  :  0.012565437
loss  111  :  0.011597209
loss  112  :  0.015361868
loss  113  :  0.011965196
loss  114  :  0.0134733375
loss  115  :  0.013825292
loss  116  :  0.0129276905
loss  117  :  0.01223735
loss  118  :  0.017697772
loss  119  :  0.013217377
loss  120  :  0.012678435
loss  121  :  0.01163947
loss  122  :  0.012758002
loss  123  :  0.0117797535
loss  124  :  0.014295554
loss  125  :  0.011527954
loss  126  :  0.011207623
loss  127  :  0.013987196
loss  128  :  0.0112468805
loss  129  :  0.013079765
loss  130  :  0.011226429
loss  131  :  0.011618663
loss  132  :  0.011252554
loss  133  :  0.013849375
loss  134  :  0.013808308
loss  135  :  0.011817704
loss  136  :  0.014752599
loss  137  :  0.012558842
loss  138  :  0.012424538
loss  139  :  0.011614806
loss  140  :  0.01413623
loss  141  :  0.012219375
loss  142  :  0.012414116
loss  143  :  0.012842974
loss  144  :  0.014080691
loss  145  :  0.013866546
loss  146  :  0.014829926
loss  147  :  0.014191553
loss  148  :  0.013158972
loss  149  :  0.011254544
loss  150  :  0.013274227
loss  151  :  0.0128609445
loss  152  :  0.011826193
loss  153  :  0.01063483
loss  154  :  0.012985712
loss  155  :  0.0112828985
loss  156  :  0.012340458
loss  157  :  0.010785191
loss  158  :  0.012463353
loss  159  :  0.012371985
loss  160  :  0.009861958
loss  161  :  0.009635055
loss  162  :  0.012120403
loss  163  :  0.0145535
loss  164  :  0.009599068
loss  165  :  0.010486607
loss  166  :  0.013362895
loss  167  :  0.012183407
loss  168  :  0.013743398
loss  169  :  0.013483477
loss  170  :  0.010967788
loss  171  :  0.011606684
loss  172  :  0.01138961
loss  173  :  0.010903971
loss  174  :  0.013707101
loss  175  :  0.0106495805
loss  176  :  0.010501085
loss  177  :  0.01181857
loss  178  :  0.010400796
loss  179  :  0.013809515
loss  180  :  0.012885362
loss  181  :  0.014691028
loss  182  :  0.01126371
loss  183  :  0.01598112
loss  184  :  0.012378685
loss  185  :  0.0105606355
loss  186  :  0.013870952
loss  187  :  0.013974076
loss  188  :  0.014340433
loss  189  :  0.012077141
loss  190  :  0.010873514
loss  191  :  0.014559291
loss  192  :  0.011442179
loss  193  :  0.013595629
loss  194  :  0.0126934545
loss  195  :  0.012065972
loss  196  :  0.012150207
loss  197  :  0.013327679
loss  198  :  0.011543481
loss  199  :  0.012292605
loss  200  :  0.01226139
loss  201  :  0.016073667
loss  202  :  0.011978038
loss  203  :  0.014373166
loss  204  :  0.012172149
loss  205  :  0.0146537395
loss  206  :  0.012686467
loss  207  :  0.016200691
loss  208  :  0.0109163625
loss  209  :  0.011158711
loss  210  :  0.01210089
loss  211  :  0.013840499
loss  212  :  0.013206142
loss  213  :  0.012034861
loss  214  :  0.011405317
loss  215  :  0.012561182
loss  216  :  0.010921074
loss  217  :  0.0134161785
loss  218  :  0.011338284
loss  219  :  0.020110626
loss  220  :  0.011488331
loss  221  :  0.010270591
loss  222  :  0.0111748
loss  223  :  0.016275043
loss  224  :  0.011743966
loss  225  :  0.0125116315
loss  226  :  0.012274001
loss  227  :  0.012682972
loss  228  :  0.013168618
loss  229  :  0.0122102965
loss  230  :  0.011988771
loss  231  :  0.011330041
loss  232  :  0.013899389
loss  233  :  0.011896828
loss  234  :  0.012897362
loss  235  :  0.012708192
loss  236  :  0.011236104
loss  237  :  0.012520562
loss  238  :  0.012168288
loss  239  :  0.011255903
loss  240  :  0.010980295
loss  241  :  0.011581527
loss  242  :  0.013109843
loss  243  :  0.012519565
loss  244  :  0.020575147
loss  245  :  0.011116715
loss  246  :  0.010727
loss  247  :  0.012753594
loss  248  :  0.010243696
loss  249  :  0.012134448
loss  250  :  0.013695042
loss  251  :  0.010997539
loss  252  :  0.0103928195
loss  253  :  0.011303106
loss  254  :  0.0118505005
loss  255  :  0.009563249
loss  256  :  0.011146744
loss  257  :  0.011590192
loss  258  :  0.009615606
loss  259  :  0.010930222
paths_onpol:  0  running.....
data buffer size:  50000
total return:  935.8584198809746
costs:  -1032.9865104293228
paths_onpol:  1  running.....
data buffer size:  51000
total return:  997.9753857240887
costs:  -1203.7916154757513
paths_onpol:  2  running.....
data buffer size:  52000
total return:  995.9599200590022
costs:  -1137.9581276991373
paths_onpol:  3  running.....
data buffer size:  53000
total return:  1019.3602431817716
costs:  -1159.8136017324398
paths_onpol:  4  running.....
data buffer size:  54000
total return:  938.6905792204033
costs:  -1081.455301214613
paths_onpol:  5  running.....
data buffer size:  55000
total return:  871.9874280115629
costs:  -1037.2476627197818
paths_onpol:  6  running.....
data buffer size:  56000
total return:  1000.1668734910583
costs:  -1177.0022528924144
paths_onpol:  7  running.....
data buffer size:  57000
total return:  1023.7212231346994
costs:  -1127.3647489849216
paths_onpol:  8  running.....
data buffer size:  58000
total return:  957.3315145955206
costs:  -1118.6981375784605
paths_onpol:  9  running.....
data buffer size:  59000
total return:  958.4736974083312
costs:  -1099.9729300081128
-------------------------------------
|       Iteration |               4 |
|     AverageCost |       -1.12e+03 |
|         StdCost |            53.4 |
|     MinimumCost |        -1.2e+03 |
|     MaximumCost |       -1.03e+03 |
|   AverageReturn |             970 |
|       StdReturn |            44.4 |
|   MinimumReturn |             872 |
|   MaximumReturn |        1.02e+03 |
-------------------------------------
onpol_iters:  5
Model fitting for  260 times ... 
loss  0  :  0.012331078
loss  1  :  0.016244803
loss  2  :  0.01573899
loss  3  :  0.014278844
loss  4  :  0.013002005
loss  5  :  0.013392101
loss  6  :  0.014064938
loss  7  :  0.012466295
loss  8  :  0.017928412
loss  9  :  0.015912559
loss  10  :  0.011190715
loss  11  :  0.014108449
loss  12  :  0.013103992
loss  13  :  0.012491735
loss  14  :  0.015155601
loss  15  :  0.011524858
loss  16  :  0.01749296
loss  17  :  0.017485458
loss  18  :  0.013772582
loss  19  :  0.015077388
loss  20  :  0.016419344
loss  21  :  0.011554157
loss  22  :  0.015279119
loss  23  :  0.013089715
loss  24  :  0.013728839
loss  25  :  0.013232915
loss  26  :  0.010749408
loss  27  :  0.013708386
loss  28  :  0.013135376
loss  29  :  0.013775256
loss  30  :  0.0136573445
loss  31  :  0.013643789
loss  32  :  0.015750086
loss  33  :  0.0124558
loss  34  :  0.013433722
loss  35  :  0.012724273
loss  36  :  0.01610909
loss  37  :  0.014873719
loss  38  :  0.0138626695
loss  39  :  0.012024624
loss  40  :  0.010807379
loss  41  :  0.01369639
loss  42  :  0.013238479
loss  43  :  0.014018482
loss  44  :  0.012404569
loss  45  :  0.009825245
loss  46  :  0.011651801
loss  47  :  0.012709099
loss  48  :  0.011684121
loss  49  :  0.012998873
loss  50  :  0.012608136
loss  51  :  0.013598949
loss  52  :  0.011674264
loss  53  :  0.013486171
loss  54  :  0.014026356
loss  55  :  0.009938862
loss  56  :  0.013855132
loss  57  :  0.0125077935
loss  58  :  0.012491984
loss  59  :  0.013953954
loss  60  :  0.011464188
loss  61  :  0.01241567
loss  62  :  0.012198477
loss  63  :  0.01416032
loss  64  :  0.012202841
loss  65  :  0.011151175
loss  66  :  0.01222716
loss  67  :  0.014640665
loss  68  :  0.013433951
loss  69  :  0.011118393
loss  70  :  0.013522899
loss  71  :  0.012257276
loss  72  :  0.013397327
loss  73  :  0.012954667
loss  74  :  0.012714526
loss  75  :  0.014928674
loss  76  :  0.018230412
loss  77  :  0.014599127
loss  78  :  0.0117538385
loss  79  :  0.015462289
loss  80  :  0.013747972
loss  81  :  0.013697793
loss  82  :  0.016661417
loss  83  :  0.011587165
loss  84  :  0.014204112
loss  85  :  0.0145880375
loss  86  :  0.014748538
loss  87  :  0.014480472
loss  88  :  0.012337158
loss  89  :  0.013500376
loss  90  :  0.010556676
loss  91  :  0.010838473
loss  92  :  0.011382378
loss  93  :  0.0137900235
loss  94  :  0.01148794
loss  95  :  0.01149473
loss  96  :  0.012274622
loss  97  :  0.012178515
loss  98  :  0.01101134
loss  99  :  0.012844664
loss  100  :  0.012981248
loss  101  :  0.011393371
loss  102  :  0.012765269
loss  103  :  0.015363961
loss  104  :  0.010277586
loss  105  :  0.012101991
loss  106  :  0.015151243
loss  107  :  0.013054815
loss  108  :  0.01564395
loss  109  :  0.011817487
loss  110  :  0.012669238
loss  111  :  0.014046064
loss  112  :  0.012820734
loss  113  :  0.012799069
loss  114  :  0.012335494
loss  115  :  0.013422462
loss  116  :  0.014381468
loss  117  :  0.016481435
loss  118  :  0.012067645
loss  119  :  0.011088948
loss  120  :  0.013156442
loss  121  :  0.013682474
loss  122  :  0.011227684
loss  123  :  0.01222333
loss  124  :  0.009627033
loss  125  :  0.012834554
loss  126  :  0.012287964
loss  127  :  0.012490297
loss  128  :  0.010545601
loss  129  :  0.017608423
loss  130  :  0.010358385
loss  131  :  0.014060421
loss  132  :  0.012229936
loss  133  :  0.01158385
loss  134  :  0.015283431
loss  135  :  0.011179468
loss  136  :  0.0118637225
loss  137  :  0.010347946
loss  138  :  0.011065794
loss  139  :  0.011051166
loss  140  :  0.012638509
loss  141  :  0.011925383
loss  142  :  0.01314188
loss  143  :  0.013012221
loss  144  :  0.010821529
loss  145  :  0.010397818
loss  146  :  0.016126469
loss  147  :  0.01271266
loss  148  :  0.011221403
loss  149  :  0.013563944
loss  150  :  0.011590037
loss  151  :  0.010704597
loss  152  :  0.011739585
loss  153  :  0.01153917
loss  154  :  0.010868757
loss  155  :  0.013615483
loss  156  :  0.010107791
loss  157  :  0.011619384
loss  158  :  0.011346087
loss  159  :  0.010915788
loss  160  :  0.012093158
loss  161  :  0.009891468
loss  162  :  0.013199026
loss  163  :  0.010829696
loss  164  :  0.01302136
loss  165  :  0.012197796
loss  166  :  0.0108584445
loss  167  :  0.011696199
loss  168  :  0.011841184
loss  169  :  0.014490342
loss  170  :  0.017048424
loss  171  :  0.010382644
loss  172  :  0.01493946
loss  173  :  0.011719109
loss  174  :  0.0107845925
loss  175  :  0.010620638
loss  176  :  0.009380192
loss  177  :  0.010462442
loss  178  :  0.012636584
loss  179  :  0.011193796
loss  180  :  0.0097157555
loss  181  :  0.011588629
loss  182  :  0.012703478
loss  183  :  0.013003282
loss  184  :  0.01008361
loss  185  :  0.013831047
loss  186  :  0.01168475
loss  187  :  0.011923464
loss  188  :  0.01032002
loss  189  :  0.012950191
loss  190  :  0.010352685
loss  191  :  0.011293429
loss  192  :  0.010632535
loss  193  :  0.010002264
loss  194  :  0.012056055
loss  195  :  0.009924848
loss  196  :  0.012804225
loss  197  :  0.011059923
loss  198  :  0.010685411
loss  199  :  0.0096067665
loss  200  :  0.010635089
loss  201  :  0.012820902
loss  202  :  0.010606358
loss  203  :  0.014492216
loss  204  :  0.009743514
loss  205  :  0.012214781
loss  206  :  0.013360051
loss  207  :  0.014680135
loss  208  :  0.01306321
loss  209  :  0.012813231
loss  210  :  0.012961592
loss  211  :  0.012456035
loss  212  :  0.012442598
loss  213  :  0.014680562
loss  214  :  0.014846465
loss  215  :  0.012114421
loss  216  :  0.011369081
loss  217  :  0.014857379
loss  218  :  0.014027113
loss  219  :  0.011571968
loss  220  :  0.010294473
loss  221  :  0.012199627
loss  222  :  0.011345086
loss  223  :  0.016429065
loss  224  :  0.013749743
loss  225  :  0.011374833
loss  226  :  0.011382077
loss  227  :  0.010706592
loss  228  :  0.011508884
loss  229  :  0.0125996545
loss  230  :  0.014693159
loss  231  :  0.011690532
loss  232  :  0.0115213
loss  233  :  0.011462048
loss  234  :  0.0131854
loss  235  :  0.0109281195
loss  236  :  0.0148017835
loss  237  :  0.013238552
loss  238  :  0.012632367
loss  239  :  0.010819633
loss  240  :  0.011435611
loss  241  :  0.012896478
loss  242  :  0.009608072
loss  243  :  0.013165308
loss  244  :  0.0106678475
loss  245  :  0.010154833
loss  246  :  0.012315276
loss  247  :  0.011132404
loss  248  :  0.011377366
loss  249  :  0.014674686
loss  250  :  0.0097440425
loss  251  :  0.010924066
loss  252  :  0.010834875
loss  253  :  0.009846015
loss  254  :  0.011226716
loss  255  :  0.011333882
loss  256  :  0.0102357315
loss  257  :  0.011814479
loss  258  :  0.012159831
loss  259  :  0.0132674845
paths_onpol:  0  running.....
data buffer size:  60000
total return:  956.7797857339139
costs:  -1026.971305497418
paths_onpol:  1  running.....
data buffer size:  61000
total return:  1091.8306557453895
costs:  -1121.8908153388484
paths_onpol:  2  running.....
data buffer size:  62000
total return:  1117.9043430891984
costs:  -1160.4579697154722
paths_onpol:  3  running.....
data buffer size:  63000
total return:  1073.939514061021
costs:  -1166.3585350675492
paths_onpol:  4  running.....
data buffer size:  64000
total return:  1100.8736646426232
costs:  -1172.8159492412867
paths_onpol:  5  running.....
data buffer size:  65000
total return:  1164.5247208448297
costs:  -1232.5695555873833
paths_onpol:  6  running.....
data buffer size:  66000
total return:  1205.2651877000785
costs:  -1331.0553620563858
paths_onpol:  7  running.....
data buffer size:  67000
total return:  1151.075628502151
costs:  -1264.7335164820634
paths_onpol:  8  running.....
data buffer size:  68000
total return:  1122.2717483458653
costs:  -1265.431136262301
paths_onpol:  9  running.....
data buffer size:  69000
total return:  1147.9695422121981
costs:  -1249.7312328588082
-------------------------------------
|       Iteration |               5 |
|     AverageCost |        -1.2e+03 |
|         StdCost |            82.9 |
|     MinimumCost |       -1.33e+03 |
|     MaximumCost |       -1.03e+03 |
|   AverageReturn |        1.11e+03 |
|       StdReturn |            63.6 |
|   MinimumReturn |             957 |
|   MaximumReturn |        1.21e+03 |
-------------------------------------
onpol_iters:  6
Model fitting for  260 times ... 
loss  0  :  0.010832591
loss  1  :  0.013779844
loss  2  :  0.010154514
loss  3  :  0.014462598
loss  4  :  0.013260899
loss  5  :  0.012231131
loss  6  :  0.011636978
loss  7  :  0.011378955
loss  8  :  0.014163807
loss  9  :  0.012551749
loss  10  :  0.01233382
loss  11  :  0.014135304
loss  12  :  0.0134767415
loss  13  :  0.011721147
loss  14  :  0.013295978
loss  15  :  0.012390176
loss  16  :  0.011778623
loss  17  :  0.014730385
loss  18  :  0.011878004
loss  19  :  0.012405958
loss  20  :  0.009933084
loss  21  :  0.010039166
loss  22  :  0.011350286
loss  23  :  0.011268141
loss  24  :  0.010701105
loss  25  :  0.012314906
loss  26  :  0.01206893
loss  27  :  0.010344522
loss  28  :  0.013318315
loss  29  :  0.012837027
loss  30  :  0.011320298
loss  31  :  0.009281489
loss  32  :  0.012269376
loss  33  :  0.011199096
loss  34  :  0.011608224
loss  35  :  0.010217726
loss  36  :  0.01198536
loss  37  :  0.009961464
loss  38  :  0.009670989
loss  39  :  0.018280286
loss  40  :  0.010698127
loss  41  :  0.014160767
loss  42  :  0.010926643
loss  43  :  0.012627767
loss  44  :  0.013884355
loss  45  :  0.011995947
loss  46  :  0.013289938
loss  47  :  0.011319799
loss  48  :  0.0179375
loss  49  :  0.011131625
loss  50  :  0.010966758
loss  51  :  0.011613418
loss  52  :  0.0111526605
loss  53  :  0.012077393
loss  54  :  0.011734181
loss  55  :  0.014016139
loss  56  :  0.0127660725
loss  57  :  0.012158449
loss  58  :  0.01234589
loss  59  :  0.010574082
loss  60  :  0.010631393
loss  61  :  0.011267662
loss  62  :  0.015772238
loss  63  :  0.012167612
loss  64  :  0.011668501
loss  65  :  0.012267606
loss  66  :  0.0121963555
loss  67  :  0.013349926
loss  68  :  0.015527752
loss  69  :  0.009896306
loss  70  :  0.011646383
loss  71  :  0.014483245
loss  72  :  0.016534524
loss  73  :  0.014845079
loss  74  :  0.011753768
loss  75  :  0.011030005
loss  76  :  0.012106415
loss  77  :  0.011055993
loss  78  :  0.011585563
loss  79  :  0.011185551
loss  80  :  0.011733426
loss  81  :  0.0110614095
loss  82  :  0.011349553
loss  83  :  0.01118329
loss  84  :  0.0108805625
loss  85  :  0.009855677
loss  86  :  0.0116322525
loss  87  :  0.011602673
loss  88  :  0.015550474
loss  89  :  0.013127329
loss  90  :  0.010983032
loss  91  :  0.010751915
loss  92  :  0.012102482
loss  93  :  0.012213716
loss  94  :  0.013033444
loss  95  :  0.013162483
loss  96  :  0.011883271
loss  97  :  0.012937358
loss  98  :  0.011015293
loss  99  :  0.010128235
loss  100  :  0.011716117
loss  101  :  0.01183621
loss  102  :  0.011832118
loss  103  :  0.011622297
loss  104  :  0.0114476755
loss  105  :  0.011466953
loss  106  :  0.010696669
loss  107  :  0.01010965
loss  108  :  0.010242129
loss  109  :  0.010124118
loss  110  :  0.009344911
loss  111  :  0.0112361275
loss  112  :  0.011514107
loss  113  :  0.01094765
loss  114  :  0.012195026
loss  115  :  0.010537893
loss  116  :  0.011247516
loss  117  :  0.016035205
loss  118  :  0.015664767
loss  119  :  0.012497732
loss  120  :  0.0143801365
loss  121  :  0.015355517
loss  122  :  0.014151146
loss  123  :  0.013252321
loss  124  :  0.014474305
loss  125  :  0.014586711
loss  126  :  0.012431276
loss  127  :  0.0101888925
loss  128  :  0.0110237645
loss  129  :  0.0119064795
loss  130  :  0.0121005
loss  131  :  0.009753409
loss  132  :  0.012025214
loss  133  :  0.013343582
loss  134  :  0.012544306
loss  135  :  0.0143315
loss  136  :  0.0123170195
loss  137  :  0.014627519
loss  138  :  0.013406882
loss  139  :  0.013407573
loss  140  :  0.013295102
loss  141  :  0.014983004
loss  142  :  0.010752303
loss  143  :  0.011858566
loss  144  :  0.010658482
loss  145  :  0.014757271
loss  146  :  0.010354785
loss  147  :  0.011831514
loss  148  :  0.01133116
loss  149  :  0.0092878435
loss  150  :  0.0115998285
loss  151  :  0.010030888
loss  152  :  0.011021546
loss  153  :  0.013436781
loss  154  :  0.011962374
loss  155  :  0.011179665
loss  156  :  0.014424322
loss  157  :  0.011281219
loss  158  :  0.013244392
loss  159  :  0.014923046
loss  160  :  0.010384217
loss  161  :  0.016131597
loss  162  :  0.0105677415
loss  163  :  0.011987005
loss  164  :  0.0129214395
loss  165  :  0.012624647
loss  166  :  0.010510992
loss  167  :  0.009593354
loss  168  :  0.010334767
loss  169  :  0.012660855
loss  170  :  0.012012787
loss  171  :  0.011892306
loss  172  :  0.012771358
loss  173  :  0.013684353
loss  174  :  0.0089962315
loss  175  :  0.010824141
loss  176  :  0.011578749
loss  177  :  0.009509457
loss  178  :  0.012521741
loss  179  :  0.013463381
loss  180  :  0.011688245
loss  181  :  0.009451869
loss  182  :  0.010631767
loss  183  :  0.011665161
loss  184  :  0.010206735
loss  185  :  0.010978617
loss  186  :  0.012637858
loss  187  :  0.012221183
loss  188  :  0.011073221
loss  189  :  0.010232093
loss  190  :  0.011079334
loss  191  :  0.010930951
loss  192  :  0.012336357
loss  193  :  0.008816615
loss  194  :  0.011565881
loss  195  :  0.010301357
loss  196  :  0.010033126
loss  197  :  0.011169575
loss  198  :  0.011642818
loss  199  :  0.0090884445
loss  200  :  0.010276839
loss  201  :  0.010243722
loss  202  :  0.01130113
loss  203  :  0.01061655
loss  204  :  0.010797083
loss  205  :  0.01038436
loss  206  :  0.010402602
loss  207  :  0.010712035
loss  208  :  0.011350404
loss  209  :  0.008491497
loss  210  :  0.010414758
loss  211  :  0.010154245
loss  212  :  0.010846345
loss  213  :  0.010754424
loss  214  :  0.0101889875
loss  215  :  0.01072221
loss  216  :  0.010208681
loss  217  :  0.00974564
loss  218  :  0.011283916
loss  219  :  0.0102346735
loss  220  :  0.011834048
loss  221  :  0.013588456
loss  222  :  0.011453761
loss  223  :  0.0104552
loss  224  :  0.010277027
loss  225  :  0.009817705
loss  226  :  0.011988556
loss  227  :  0.010204494
loss  228  :  0.010457907
loss  229  :  0.012919232
loss  230  :  0.010234033
loss  231  :  0.010981046
loss  232  :  0.008843115
loss  233  :  0.011540292
loss  234  :  0.012435864
loss  235  :  0.009220186
loss  236  :  0.009488082
loss  237  :  0.014212826
loss  238  :  0.010877212
loss  239  :  0.012572211
loss  240  :  0.01158883
loss  241  :  0.0126622515
loss  242  :  0.012439695
loss  243  :  0.010349093
loss  244  :  0.012222528
loss  245  :  0.012509264
loss  246  :  0.012396591
loss  247  :  0.0147458315
loss  248  :  0.011718318
loss  249  :  0.014381861
loss  250  :  0.015769074
loss  251  :  0.01180589
loss  252  :  0.014619693
loss  253  :  0.0129084755
loss  254  :  0.009874773
loss  255  :  0.011687103
loss  256  :  0.013455061
loss  257  :  0.01388863
loss  258  :  0.011184676
loss  259  :  0.010299333
paths_onpol:  0  running.....
data buffer size:  70000
total return:  1023.2742158981556
costs:  -1122.9565070140768
paths_onpol:  1  running.....
data buffer size:  71000
total return:  1149.8224438692203
costs:  -1238.5902147766492
paths_onpol:  2  running.....
data buffer size:  72000
total return:  1035.188965795704
costs:  -1108.2150106074077
paths_onpol:  3  running.....
data buffer size:  73000
total return:  1110.6111152501642
costs:  -1190.630136433885
paths_onpol:  4  running.....
data buffer size:  74000
total return:  1106.735445839887
costs:  -1291.7771257882125
paths_onpol:  5  running.....
data buffer size:  75000
total return:  1029.0469455914715
costs:  -1192.2256636274653
paths_onpol:  6  running.....
data buffer size:  76000
total return:  1055.8675257354741
costs:  -1238.15562078572
paths_onpol:  7  running.....
data buffer size:  77000
total return:  1060.4741207100917
costs:  -1190.8749155811315
paths_onpol:  8  running.....
data buffer size:  78000
total return:  65.35305745530945
costs:  23567.48853966179
paths_onpol:  9  running.....
data buffer size:  79000
total return:  1029.7272548001642
costs:  -1076.9457239451203
-------------------------------------
|       Iteration |               6 |
|     AverageCost |        1.29e+03 |
|         StdCost |        7.43e+03 |
|     MinimumCost |       -1.29e+03 |
|     MaximumCost |        2.36e+04 |
|   AverageReturn |             967 |
|       StdReturn |             303 |
|   MinimumReturn |            65.4 |
|   MaximumReturn |        1.15e+03 |
-------------------------------------
onpol_iters:  7
Model fitting for  260 times ... 
loss  0  :  3.972432
loss  1  :  4.110965
loss  2  :  0.94089144
loss  3  :  0.50210583
loss  4  :  0.36072612
loss  5  :  0.8853275
loss  6  :  0.6745323
loss  7  :  0.7073955
loss  8  :  0.33157715
loss  9  :  0.24407585
loss  10  :  0.16553259
loss  11  :  0.16513278
loss  12  :  0.17836396
loss  13  :  0.27767128
loss  14  :  0.20598504
loss  15  :  0.1524562
loss  16  :  0.15460224
loss  17  :  0.12646712
loss  18  :  0.108247854
loss  19  :  0.098319866
loss  20  :  0.12401366
loss  21  :  0.097918086
loss  22  :  0.09513025
loss  23  :  0.08099745
loss  24  :  0.08872597
loss  25  :  0.07796995
loss  26  :  0.07055084
loss  27  :  0.06870745
loss  28  :  0.0697126
loss  29  :  0.06725027
loss  30  :  0.0707727
loss  31  :  0.06031266
loss  32  :  0.059350707
loss  33  :  0.05320277
loss  34  :  0.052446425
loss  35  :  0.056153
loss  36  :  0.055315305
loss  37  :  0.054595042
loss  38  :  0.05704689
loss  39  :  0.04312046
loss  40  :  0.052481662
loss  41  :  0.048811097
loss  42  :  0.047810405
loss  43  :  0.05772195
loss  44  :  0.04763961
loss  45  :  0.05053079
loss  46  :  0.047910713
loss  47  :  0.045282222
loss  48  :  0.043113314
loss  49  :  0.041317265
loss  50  :  0.041592695
loss  51  :  0.03494848
loss  52  :  0.036483694
loss  53  :  0.035563428
loss  54  :  0.038135745
loss  55  :  0.038933072
loss  56  :  0.03760881
loss  57  :  0.03884241
loss  58  :  0.040977173
loss  59  :  0.032001354
loss  60  :  0.034689438
loss  61  :  0.033694275
loss  62  :  0.03880722
loss  63  :  0.036176197
loss  64  :  0.040247254
loss  65  :  0.037473686
loss  66  :  0.03296446
loss  67  :  0.034455705
loss  68  :  0.03707413
loss  69  :  0.035857897
loss  70  :  0.03188335
loss  71  :  0.036456894
loss  72  :  0.03459817
loss  73  :  0.038687356
loss  74  :  0.04004642
loss  75  :  0.035032682
loss  76  :  0.033244982
loss  77  :  0.032559305
loss  78  :  0.03252107
loss  79  :  0.028619695
loss  80  :  0.0284271
loss  81  :  0.03676507
loss  82  :  0.037343405
loss  83  :  0.044675194
loss  84  :  0.030067354
loss  85  :  0.029125407
loss  86  :  0.031837165
loss  87  :  0.028995175
loss  88  :  0.031013343
loss  89  :  0.034407604
loss  90  :  0.031515937
loss  91  :  0.029727962
loss  92  :  0.027002316
loss  93  :  0.029337306
loss  94  :  0.03983646
loss  95  :  0.035516363
loss  96  :  0.029953366
loss  97  :  0.033681598
loss  98  :  0.028340762
loss  99  :  0.026111286
loss  100  :  0.024988918
loss  101  :  0.02746814
loss  102  :  0.03275741
loss  103  :  0.036373734
loss  104  :  0.02415855
loss  105  :  0.031395137
loss  106  :  0.029853996
loss  107  :  0.027242113
loss  108  :  0.027671168
loss  109  :  0.027525846
loss  110  :  0.026829874
loss  111  :  0.024483982
loss  112  :  0.025653904
loss  113  :  0.026367962
loss  114  :  0.024088273
loss  115  :  0.029230034
loss  116  :  0.033124212
loss  117  :  0.024909293
loss  118  :  0.027180573
loss  119  :  0.030207878
loss  120  :  0.030623268
loss  121  :  0.036337335
loss  122  :  0.023756476
loss  123  :  0.02893073
loss  124  :  0.02485588
loss  125  :  0.029074878
loss  126  :  0.030130967
loss  127  :  0.027154779
loss  128  :  0.02521894
loss  129  :  0.023739416
loss  130  :  0.028070137
loss  131  :  0.021902436
loss  132  :  0.023456816
loss  133  :  0.02392731
loss  134  :  0.025244748
loss  135  :  0.026263237
loss  136  :  0.027768373
loss  137  :  0.023531744
loss  138  :  0.027962524
loss  139  :  0.023049515
loss  140  :  0.025434235
loss  141  :  0.026298925
loss  142  :  0.024090342
loss  143  :  0.023233313
loss  144  :  0.023763547
loss  145  :  0.02158643
loss  146  :  0.024983605
loss  147  :  0.028262723
loss  148  :  0.023850929
loss  149  :  0.022339318
loss  150  :  0.022031372
loss  151  :  0.025811706
loss  152  :  0.027752072
loss  153  :  0.024906963
loss  154  :  0.023433367
loss  155  :  0.026673969
loss  156  :  0.02215925
loss  157  :  0.025407007
loss  158  :  0.026596656
loss  159  :  0.018666536
loss  160  :  0.02509661
loss  161  :  0.024505429
loss  162  :  0.027149612
loss  163  :  0.025337059
loss  164  :  0.02343284
loss  165  :  0.026909059
loss  166  :  0.023476953
loss  167  :  0.024408448
loss  168  :  0.02341048
loss  169  :  0.022282604
loss  170  :  0.017754301
loss  171  :  0.023146903
loss  172  :  0.02670474
loss  173  :  0.026236523
loss  174  :  0.026021218
loss  175  :  0.028955245
loss  176  :  0.02355657
loss  177  :  0.027603403
loss  178  :  0.026555318
loss  179  :  0.030112784
loss  180  :  0.025924483
loss  181  :  0.029927757
loss  182  :  0.024164882
loss  183  :  0.023350978
loss  184  :  0.0232861
loss  185  :  0.027295783
loss  186  :  0.022921178
loss  187  :  0.024528274
loss  188  :  0.019393459
loss  189  :  0.019158978
loss  190  :  0.02043507
loss  191  :  0.027723199
loss  192  :  0.020449769
loss  193  :  0.020800075
loss  194  :  0.021745553
loss  195  :  0.02463073
loss  196  :  0.024595622
loss  197  :  0.022312578
loss  198  :  0.023520162
loss  199  :  0.020306867
loss  200  :  0.02262629
loss  201  :  0.024710914
loss  202  :  0.028505206
loss  203  :  0.02090418
loss  204  :  0.021339707
loss  205  :  0.020216018
loss  206  :  0.020968039
loss  207  :  0.022475505
loss  208  :  0.019880151
loss  209  :  0.020820035
loss  210  :  0.024325967
loss  211  :  0.023107145
loss  212  :  0.023626458
loss  213  :  0.020684302
loss  214  :  0.021024514
loss  215  :  0.021094922
loss  216  :  0.023586482
loss  217  :  0.019129071
loss  218  :  0.025276184
loss  219  :  0.023019206
loss  220  :  0.026205111
loss  221  :  0.025185233
loss  222  :  0.024053575
loss  223  :  0.02147838
loss  224  :  0.023929924
loss  225  :  0.019438434
loss  226  :  0.017298719
loss  227  :  0.020891253
loss  228  :  0.02483255
loss  229  :  0.020548463
loss  230  :  0.030294057
loss  231  :  0.018069323
loss  232  :  0.02199787
loss  233  :  0.019696921
loss  234  :  0.019200068
loss  235  :  0.016353164
loss  236  :  0.019473141
loss  237  :  0.019975327
loss  238  :  0.016755069
loss  239  :  0.024217214
loss  240  :  0.019197997
loss  241  :  0.01851456
loss  242  :  0.02070476
loss  243  :  0.015014616
loss  244  :  0.02034034
loss  245  :  0.022703027
loss  246  :  0.017982215
loss  247  :  0.02047667
loss  248  :  0.01787141
loss  249  :  0.018711796
loss  250  :  0.017786736
loss  251  :  0.020910194
loss  252  :  0.015848963
loss  253  :  0.018138375
loss  254  :  0.01838931
loss  255  :  0.017804455
loss  256  :  0.019453263
loss  257  :  0.021932092
loss  258  :  0.016084846
loss  259  :  0.02049525
paths_onpol:  0  running.....
data buffer size:  80000
total return:  1040.3108669051173
costs:  -1152.8008690707982
paths_onpol:  1  running.....
data buffer size:  81000
total return:  1107.5502297596656
costs:  -1150.8576520295055
paths_onpol:  2  running.....
data buffer size:  82000
total return:  1138.0705380061397
costs:  -1297.1643971677152
paths_onpol:  3  running.....
data buffer size:  83000
total return:  1025.705091559194
costs:  -1121.2643279304787
paths_onpol:  4  running.....
data buffer size:  84000
total return:  1172.8690554500404
costs:  -1326.061680801378
paths_onpol:  5  running.....
data buffer size:  85000
total return:  1099.9443236562117
costs:  -1192.8593003300389
paths_onpol:  6  running.....
data buffer size:  86000
total return:  1000.2692218290189
costs:  -1104.1424664018964
paths_onpol:  7  running.....
data buffer size:  87000
total return:  1128.419715055148
costs:  -1214.0710322478449
paths_onpol:  8  running.....
data buffer size:  88000
total return:  1107.1999751983792
costs:  -1126.9164950348181
paths_onpol:  9  running.....
data buffer size:  89000
total return:  1034.674753669328
costs:  -1104.3124012301203
-------------------------------------
|       Iteration |               7 |
|     AverageCost |       -1.18e+03 |
|         StdCost |            74.6 |
|     MinimumCost |       -1.33e+03 |
|     MaximumCost |        -1.1e+03 |
|   AverageReturn |        1.09e+03 |
|       StdReturn |            53.7 |
|   MinimumReturn |           1e+03 |
|   MaximumReturn |        1.17e+03 |
-------------------------------------
onpol_iters:  8
Model fitting for  260 times ... 
loss  0  :  0.017985065
loss  1  :  0.017147237
loss  2  :  0.018437384
loss  3  :  0.022964444
loss  4  :  0.017123822
loss  5  :  0.015721258
loss  6  :  0.019055527
loss  7  :  0.013875348
loss  8  :  0.019921593
loss  9  :  0.020437818
loss  10  :  0.017822715
loss  11  :  0.018881142
loss  12  :  0.017897252
loss  13  :  0.019742418
loss  14  :  0.016802605
loss  15  :  0.020583361
loss  16  :  0.022245815
loss  17  :  0.021931898
loss  18  :  0.01794928
loss  19  :  0.016544314
loss  20  :  0.015136739
loss  21  :  0.019938782
loss  22  :  0.022843618
loss  23  :  0.024137767
loss  24  :  0.023881659
loss  25  :  0.018918226
loss  26  :  0.021597829
loss  27  :  0.021908665
loss  28  :  0.018962026
loss  29  :  0.016826164
loss  30  :  0.019721497
loss  31  :  0.023782127
loss  32  :  0.01910906
loss  33  :  0.015786061
loss  34  :  0.015664877
loss  35  :  0.017127547
loss  36  :  0.018753197
loss  37  :  0.024515044
loss  38  :  0.019826928
loss  39  :  0.019847464
loss  40  :  0.018938567
loss  41  :  0.020647788
loss  42  :  0.019071843
loss  43  :  0.016596189
loss  44  :  0.018241484
loss  45  :  0.018527826
loss  46  :  0.018018007
loss  47  :  0.018043632
loss  48  :  0.018716311
loss  49  :  0.016096756
loss  50  :  0.0147546185
loss  51  :  0.016895173
loss  52  :  0.019065859
loss  53  :  0.017247416
loss  54  :  0.018339029
loss  55  :  0.01932769
loss  56  :  0.015556561
loss  57  :  0.015920779
loss  58  :  0.020769384
loss  59  :  0.01869966
loss  60  :  0.018306263
loss  61  :  0.021901052
loss  62  :  0.016475806
loss  63  :  0.01870354
loss  64  :  0.015278699
loss  65  :  0.018315116
loss  66  :  0.025266692
loss  67  :  0.016493721
loss  68  :  0.013783989
loss  69  :  0.016430538
loss  70  :  0.016583819
loss  71  :  0.018610684
loss  72  :  0.013922441
loss  73  :  0.014758353
loss  74  :  0.016279902
loss  75  :  0.016464865
loss  76  :  0.02045475
loss  77  :  0.015976584
loss  78  :  0.013293192
loss  79  :  0.014210889
loss  80  :  0.017702464
loss  81  :  0.018447464
loss  82  :  0.014769072
loss  83  :  0.017255511
loss  84  :  0.020053253
loss  85  :  0.024772119
loss  86  :  0.019600157
loss  87  :  0.017930642
loss  88  :  0.01588935
loss  89  :  0.017967738
loss  90  :  0.017678011
loss  91  :  0.01976193
loss  92  :  0.019438084
loss  93  :  0.020187091
loss  94  :  0.01441066
loss  95  :  0.016323477
loss  96  :  0.015261796
loss  97  :  0.015843213
loss  98  :  0.017961632
loss  99  :  0.01684064
loss  100  :  0.018497339
loss  101  :  0.017133433
loss  102  :  0.016803507
loss  103  :  0.01710402
loss  104  :  0.0185494
loss  105  :  0.018230429
loss  106  :  0.017308593
loss  107  :  0.016294213
loss  108  :  0.016635096
loss  109  :  0.015869996
loss  110  :  0.015816525
loss  111  :  0.018975556
loss  112  :  0.016963238
loss  113  :  0.017191863
loss  114  :  0.01839689
loss  115  :  0.018454438
loss  116  :  0.017631467
loss  117  :  0.0148527
loss  118  :  0.017242735
loss  119  :  0.016250875
loss  120  :  0.017553251
loss  121  :  0.01595968
loss  122  :  0.013390926
loss  123  :  0.013217541
loss  124  :  0.015477645
loss  125  :  0.014785779
loss  126  :  0.015461871
loss  127  :  0.012717416
loss  128  :  0.015935373
loss  129  :  0.01600976
loss  130  :  0.015695173
loss  131  :  0.014661771
loss  132  :  0.015189928
loss  133  :  0.017454956
loss  134  :  0.016032297
loss  135  :  0.015470149
loss  136  :  0.016308988
loss  137  :  0.011804493
loss  138  :  0.015445411
loss  139  :  0.019708421
loss  140  :  0.013832388
loss  141  :  0.01571866
loss  142  :  0.013857362
loss  143  :  0.0148132
loss  144  :  0.020774433
loss  145  :  0.017296517
loss  146  :  0.018009309
loss  147  :  0.0146227
loss  148  :  0.013796657
loss  149  :  0.018149856
loss  150  :  0.01646271
loss  151  :  0.0149308
loss  152  :  0.0156110795
loss  153  :  0.015072244
loss  154  :  0.016854074
loss  155  :  0.017625276
loss  156  :  0.016359407
loss  157  :  0.016385043
loss  158  :  0.017975891
loss  159  :  0.015333751
loss  160  :  0.018833239
loss  161  :  0.014703283
loss  162  :  0.017334301
loss  163  :  0.016396655
loss  164  :  0.020780567
loss  165  :  0.014537898
loss  166  :  0.016735096
loss  167  :  0.014850187
loss  168  :  0.01777399
loss  169  :  0.01843756
loss  170  :  0.01700591
loss  171  :  0.015943432
loss  172  :  0.016695822
loss  173  :  0.016831607
loss  174  :  0.019770429
loss  175  :  0.015588934
loss  176  :  0.0144229485
loss  177  :  0.01789029
loss  178  :  0.018470043
loss  179  :  0.018407186
loss  180  :  0.016902862
loss  181  :  0.016261388
loss  182  :  0.017459374
loss  183  :  0.020476986
loss  184  :  0.015602395
loss  185  :  0.016031638
loss  186  :  0.015861476
loss  187  :  0.019036954
loss  188  :  0.015066415
loss  189  :  0.0152561385
loss  190  :  0.015297638
loss  191  :  0.014438979
loss  192  :  0.0152767
loss  193  :  0.02138746
loss  194  :  0.013432938
loss  195  :  0.016033387
loss  196  :  0.01147118
loss  197  :  0.016280686
loss  198  :  0.015382138
loss  199  :  0.014277488
loss  200  :  0.017378937
loss  201  :  0.013833332
loss  202  :  0.013662358
loss  203  :  0.016984057
loss  204  :  0.01427005
loss  205  :  0.016926376
loss  206  :  0.013777454
loss  207  :  0.015789745
loss  208  :  0.014495182
loss  209  :  0.01446723
loss  210  :  0.016454624
loss  211  :  0.01804755
loss  212  :  0.018315494
loss  213  :  0.01499781
loss  214  :  0.01318368
loss  215  :  0.014959039
loss  216  :  0.015447548
loss  217  :  0.014901256
loss  218  :  0.013442902
loss  219  :  0.016262894
loss  220  :  0.014028897
loss  221  :  0.014234884
loss  222  :  0.014944944
loss  223  :  0.0142534375
loss  224  :  0.015394779
loss  225  :  0.013687363
loss  226  :  0.014410758
loss  227  :  0.016269011
loss  228  :  0.011739688
loss  229  :  0.012765428
loss  230  :  0.012239307
loss  231  :  0.015083435
loss  232  :  0.014456794
loss  233  :  0.015452718
loss  234  :  0.014141589
loss  235  :  0.013379106
loss  236  :  0.01873902
loss  237  :  0.013904797
loss  238  :  0.016204383
loss  239  :  0.015923206
loss  240  :  0.015952203
loss  241  :  0.014062117
loss  242  :  0.018402642
loss  243  :  0.013373436
loss  244  :  0.015601695
loss  245  :  0.015597545
loss  246  :  0.012646626
loss  247  :  0.013474464
loss  248  :  0.012077113
loss  249  :  0.013871548
loss  250  :  0.016390532
loss  251  :  0.013853535
loss  252  :  0.014573263
loss  253  :  0.013153118
loss  254  :  0.015439627
loss  255  :  0.013045646
loss  256  :  0.012539951
loss  257  :  0.014903051
loss  258  :  0.011177452
loss  259  :  0.014493303
paths_onpol:  0  running.....
data buffer size:  90000
total return:  1140.357559098106
costs:  -1276.3526606019304
paths_onpol:  1  running.....
data buffer size:  91000
total return:  1129.1812339016171
costs:  -1274.6017608522534
paths_onpol:  2  running.....
data buffer size:  92000
total return:  1051.2483432617394
costs:  -1238.3221013814982
paths_onpol:  3  running.....
data buffer size:  93000
total return:  988.8712264031195
costs:  -1161.1005753154107
paths_onpol:  4  running.....
data buffer size:  94000
total return:  1039.90870117133
costs:  -1170.926097664923
paths_onpol:  5  running.....
data buffer size:  95000
total return:  935.3145086917901
costs:  -1132.1700421597461
paths_onpol:  6  running.....
data buffer size:  96000
total return:  1105.4260013500734
costs:  -1244.7249810788835
paths_onpol:  7  running.....
data buffer size:  97000
total return:  1194.9425366586502
costs:  -1352.448394303438
paths_onpol:  8  running.....
data buffer size:  98000
total return:  989.675907621298
costs:  -1163.78713750723
paths_onpol:  9  running.....
data buffer size:  99000
total return:  1107.022252590394
costs:  -1221.0246123951947
-------------------------------------
|       Iteration |               8 |
|     AverageCost |       -1.22e+03 |
|         StdCost |            64.2 |
|     MinimumCost |       -1.35e+03 |
|     MaximumCost |       -1.13e+03 |
|   AverageReturn |        1.07e+03 |
|       StdReturn |            76.9 |
|   MinimumReturn |             935 |
|   MaximumReturn |        1.19e+03 |
-------------------------------------
onpol_iters:  9
Model fitting for  260 times ... 
loss  0  :  0.017780121
loss  1  :  0.01448228
loss  2  :  0.01869095
loss  3  :  0.0116777625
loss  4  :  0.018582772
loss  5  :  0.012977755
loss  6  :  0.016164985
loss  7  :  0.01586698
loss  8  :  0.01402117
loss  9  :  0.014948508
loss  10  :  0.013524461
loss  11  :  0.015604648
loss  12  :  0.014449772
loss  13  :  0.012672816
loss  14  :  0.017411232
loss  15  :  0.014478895
loss  16  :  0.013805944
loss  17  :  0.015168434
loss  18  :  0.01681953
loss  19  :  0.013486123
loss  20  :  0.014445039
loss  21  :  0.017872516
loss  22  :  0.015679969
loss  23  :  0.015799364
loss  24  :  0.013112275
loss  25  :  0.01391416
loss  26  :  0.013871083
loss  27  :  0.013304855
loss  28  :  0.017462304
loss  29  :  0.013643277
loss  30  :  0.011236129
loss  31  :  0.017935958
loss  32  :  0.0137623325
loss  33  :  0.014188817
loss  34  :  0.014063507
loss  35  :  0.014607188
loss  36  :  0.013862863
loss  37  :  0.014227013
loss  38  :  0.014769644
loss  39  :  0.017301762
loss  40  :  0.014298225
loss  41  :  0.012859246
loss  42  :  0.015652457
loss  43  :  0.012398547
loss  44  :  0.015510865
loss  45  :  0.01529686
loss  46  :  0.014247095
loss  47  :  0.016079037
loss  48  :  0.014932044
loss  49  :  0.016665565
loss  50  :  0.011416534
loss  51  :  0.0191835
loss  52  :  0.01425527
loss  53  :  0.013002957
loss  54  :  0.013417286
loss  55  :  0.014687602
loss  56  :  0.013433121
loss  57  :  0.016123239
loss  58  :  0.019996718
loss  59  :  0.012189975
loss  60  :  0.0150875
loss  61  :  0.013783516
loss  62  :  0.012136082
loss  63  :  0.014606195
loss  64  :  0.011807631
loss  65  :  0.013564196
loss  66  :  0.012703833
loss  67  :  0.013666369
loss  68  :  0.011893766
loss  69  :  0.016700597
loss  70  :  0.018093627
loss  71  :  0.012250066
loss  72  :  0.015045904
loss  73  :  0.018441964
loss  74  :  0.015709387
loss  75  :  0.011718592
loss  76  :  0.013608068
loss  77  :  0.016186899
loss  78  :  0.013277151
loss  79  :  0.011555081
loss  80  :  0.01383287
loss  81  :  0.012767253
loss  82  :  0.015283073
loss  83  :  0.014907244
loss  84  :  0.017188286
loss  85  :  0.017018761
loss  86  :  0.017016442
loss  87  :  0.010996836
loss  88  :  0.014865093
loss  89  :  0.018541811
loss  90  :  0.016864967
loss  91  :  0.016199786
loss  92  :  0.012026381
loss  93  :  0.015978795
loss  94  :  0.014434037
loss  95  :  0.01419465
loss  96  :  0.013357552
loss  97  :  0.01612092
loss  98  :  0.015822604
loss  99  :  0.014041054
loss  100  :  0.014955254
loss  101  :  0.01522615
loss  102  :  0.011964964
loss  103  :  0.015653316
loss  104  :  0.013314882
loss  105  :  0.014744443
loss  106  :  0.012789282
loss  107  :  0.013005425
loss  108  :  0.010940159
loss  109  :  0.012753064
loss  110  :  0.018595908
loss  111  :  0.015510033
loss  112  :  0.013434063
loss  113  :  0.013984186
loss  114  :  0.014819217
loss  115  :  0.013800373
loss  116  :  0.010925004
loss  117  :  0.016874183
loss  118  :  0.016306307
loss  119  :  0.014845775
loss  120  :  0.012774664
loss  121  :  0.0144306775
loss  122  :  0.0131428335
loss  123  :  0.013333604
loss  124  :  0.011423918
loss  125  :  0.014193034
loss  126  :  0.011073625
loss  127  :  0.011250568
loss  128  :  0.011715094
loss  129  :  0.014350697
loss  130  :  0.0111616375
loss  131  :  0.014565999
loss  132  :  0.010312992
loss  133  :  0.011109467
loss  134  :  0.013387611
loss  135  :  0.01282612
loss  136  :  0.01395301
loss  137  :  0.012832647
loss  138  :  0.013464746
loss  139  :  0.013856428
loss  140  :  0.01350607
loss  141  :  0.013437977
loss  142  :  0.012364312
loss  143  :  0.011663037
loss  144  :  0.015543851
loss  145  :  0.012099872
loss  146  :  0.010902914
loss  147  :  0.014790526
loss  148  :  0.0153616695
loss  149  :  0.014858127
loss  150  :  0.015644157
loss  151  :  0.009643061
loss  152  :  0.012047795
loss  153  :  0.012681988
loss  154  :  0.014983192
loss  155  :  0.012007734
loss  156  :  0.01306468
loss  157  :  0.014414159
loss  158  :  0.011886534
loss  159  :  0.015187144
loss  160  :  0.013027869
loss  161  :  0.013068944
loss  162  :  0.011062792
loss  163  :  0.012893218
loss  164  :  0.013013294
loss  165  :  0.013579836
loss  166  :  0.014063202
loss  167  :  0.011618972
loss  168  :  0.015862433
loss  169  :  0.014141465
loss  170  :  0.016342351
loss  171  :  0.012766773
loss  172  :  0.0121376645
loss  173  :  0.011969131
loss  174  :  0.013938835
loss  175  :  0.011345494
loss  176  :  0.013129519
loss  177  :  0.011484797
loss  178  :  0.013109228
loss  179  :  0.014949518
loss  180  :  0.011761151
loss  181  :  0.013559421
loss  182  :  0.010780193
loss  183  :  0.012790153
loss  184  :  0.01631462
loss  185  :  0.011475812
loss  186  :  0.01488145
loss  187  :  0.011694493
loss  188  :  0.01326336
loss  189  :  0.013090545
loss  190  :  0.0142586855
loss  191  :  0.013063053
loss  192  :  0.011477004
loss  193  :  0.014553944
loss  194  :  0.013068239
loss  195  :  0.011431927
loss  196  :  0.013257094
loss  197  :  0.01341899
loss  198  :  0.012406116
loss  199  :  0.012511539
loss  200  :  0.010402739
loss  201  :  0.011681079
loss  202  :  0.015092251
loss  203  :  0.012037284
loss  204  :  0.01255208
loss  205  :  0.011213499
loss  206  :  0.0127766775
loss  207  :  0.012509892
loss  208  :  0.01226974
loss  209  :  0.012263274
loss  210  :  0.011303471
loss  211  :  0.011610584
loss  212  :  0.014665136
loss  213  :  0.0132986205
loss  214  :  0.009764472
loss  215  :  0.01686871
loss  216  :  0.013309389
loss  217  :  0.010781598
loss  218  :  0.011654748
loss  219  :  0.010696947
loss  220  :  0.011872118
loss  221  :  0.010396251
loss  222  :  0.011725818
loss  223  :  0.012489141
loss  224  :  0.011765832
loss  225  :  0.009925673
loss  226  :  0.014921385
loss  227  :  0.011605534
loss  228  :  0.013224517
loss  229  :  0.010676749
loss  230  :  0.015082833
loss  231  :  0.01261062
loss  232  :  0.014531004
loss  233  :  0.013325512
loss  234  :  0.01559425
loss  235  :  0.014329064
loss  236  :  0.01349387
loss  237  :  0.013510284
loss  238  :  0.012977871
loss  239  :  0.01306158
loss  240  :  0.012004947
loss  241  :  0.013052739
loss  242  :  0.013482666
loss  243  :  0.013489479
loss  244  :  0.01366476
loss  245  :  0.008732001
loss  246  :  0.011656352
loss  247  :  0.015716095
loss  248  :  0.013685152
loss  249  :  0.015094811
loss  250  :  0.012887178
loss  251  :  0.018241934
loss  252  :  0.012799871
loss  253  :  0.011197478
loss  254  :  0.0121890325
loss  255  :  0.01262719
loss  256  :  0.012611827
loss  257  :  0.016934382
loss  258  :  0.013072854
loss  259  :  0.012498548
paths_onpol:  0  running.....
data buffer size:  100000
total return:  1072.89728205667
costs:  -1090.5723511045328
paths_onpol:  1  running.....
data buffer size:  101000
total return:  1081.6400898142447
costs:  -1156.2304574064676
paths_onpol:  2  running.....
data buffer size:  102000
total return:  1063.8127153380074
costs:  -1216.0161837809032
paths_onpol:  3  running.....
data buffer size:  103000
total return:  1189.5932871959435
costs:  -1257.0172338526422
paths_onpol:  4  running.....
data buffer size:  104000
total return:  1075.613444593999
costs:  -1151.4486707269893
paths_onpol:  5  running.....
data buffer size:  105000
total return:  1089.2214137994706
costs:  -1188.0603809941026
paths_onpol:  6  running.....
data buffer size:  106000
total return:  1070.3333473046634
costs:  -1226.26058546381
paths_onpol:  7  running.....
data buffer size:  107000
total return:  1098.1124751877046
costs:  -1221.80336092607
paths_onpol:  8  running.....
data buffer size:  108000
total return:  1006.061580854076
costs:  -1064.878181654674
paths_onpol:  9  running.....
data buffer size:  109000
total return:  1070.2503313539244
costs:  -1181.3660230347402
-------------------------------------
|       Iteration |               9 |
|     AverageCost |       -1.18e+03 |
|         StdCost |              58 |
|     MinimumCost |       -1.26e+03 |
|     MaximumCost |       -1.06e+03 |
|   AverageReturn |        1.08e+03 |
|       StdReturn |            42.9 |
|   MinimumReturn |        1.01e+03 |
|   MaximumReturn |        1.19e+03 |
-------------------------------------
onpol_iters:  10
Model fitting for  260 times ... 
loss  0  :  0.012062928
loss  1  :  0.0129078
loss  2  :  0.012608941
loss  3  :  0.0135690095
loss  4  :  0.013534425
loss  5  :  0.011952793
loss  6  :  0.011617629
loss  7  :  0.0134695275
loss  8  :  0.012010934
loss  9  :  0.016140234
loss  10  :  0.011503925
loss  11  :  0.015377549
loss  12  :  0.011325309
loss  13  :  0.013353324
loss  14  :  0.011463052
loss  15  :  0.014534542
loss  16  :  0.011544809
loss  17  :  0.014515745
loss  18  :  0.014006245
loss  19  :  0.013405058
loss  20  :  0.011804544
loss  21  :  0.012478124
loss  22  :  0.015712176
loss  23  :  0.012760548
loss  24  :  0.011248795
loss  25  :  0.015016672
loss  26  :  0.01580376
loss  27  :  0.014376497
loss  28  :  0.0139699355
loss  29  :  0.011539621
loss  30  :  0.012840966
loss  31  :  0.011302279
loss  32  :  0.013398506
loss  33  :  0.012250302
loss  34  :  0.013791606
loss  35  :  0.011175023
loss  36  :  0.011705421
loss  37  :  0.012148781
loss  38  :  0.014572948
loss  39  :  0.013174
loss  40  :  0.012329027
loss  41  :  0.010852597
loss  42  :  0.011717867
loss  43  :  0.011624657
loss  44  :  0.010307891
loss  45  :  0.013418739
loss  46  :  0.0130317565
loss  47  :  0.0126092555
loss  48  :  0.01118712
loss  49  :  0.012662685
loss  50  :  0.012928603
loss  51  :  0.013397256
loss  52  :  0.01231757
loss  53  :  0.0139619615
loss  54  :  0.0120913405
loss  55  :  0.011584404
loss  56  :  0.011798787
loss  57  :  0.012343692
loss  58  :  0.01460488
loss  59  :  0.014908356
loss  60  :  0.011639206
loss  61  :  0.0144878
loss  62  :  0.016641343
loss  63  :  0.010920109
loss  64  :  0.011864908
loss  65  :  0.014376571
loss  66  :  0.0153887
loss  67  :  0.011021068
loss  68  :  0.011281402
loss  69  :  0.011655365
loss  70  :  0.011813834
loss  71  :  0.0130150495
loss  72  :  0.014780219
loss  73  :  0.01419386
loss  74  :  0.013427111
loss  75  :  0.0133040575
loss  76  :  0.011000717
loss  77  :  0.011833913
loss  78  :  0.013332519
loss  79  :  0.011899063
loss  80  :  0.015133549
loss  81  :  0.0116125755
loss  82  :  0.015052969
loss  83  :  0.023418257
loss  84  :  0.011308977
loss  85  :  0.017577134
loss  86  :  0.011404708
loss  87  :  0.012597745
loss  88  :  0.012241727
loss  89  :  0.012451549
loss  90  :  0.010910672
loss  91  :  0.010997629
loss  92  :  0.011881892
loss  93  :  0.012399507
loss  94  :  0.01166723
loss  95  :  0.013641214
loss  96  :  0.012412402
loss  97  :  0.013458073
loss  98  :  0.0129944235
loss  99  :  0.012539396
loss  100  :  0.014087851
loss  101  :  0.011351476
loss  102  :  0.011848548
loss  103  :  0.01200285
loss  104  :  0.009699592
loss  105  :  0.008569129
loss  106  :  0.0098279575
loss  107  :  0.011324858
loss  108  :  0.011484315
loss  109  :  0.010758882
loss  110  :  0.010605077
loss  111  :  0.011931749
loss  112  :  0.011958878
loss  113  :  0.01199348
loss  114  :  0.010229711
loss  115  :  0.013715856
loss  116  :  0.010664472
loss  117  :  0.011184948
loss  118  :  0.011137895
loss  119  :  0.014053258
loss  120  :  0.011100524
loss  121  :  0.010421448
loss  122  :  0.009742889
loss  123  :  0.011012482
loss  124  :  0.009869191
loss  125  :  0.010163174
loss  126  :  0.012645218
loss  127  :  0.012821829
loss  128  :  0.010757901
loss  129  :  0.011669215
loss  130  :  0.01105274
loss  131  :  0.0118871825
loss  132  :  0.011545217
loss  133  :  0.011515716
loss  134  :  0.009473803
loss  135  :  0.013985676
loss  136  :  0.010599469
loss  137  :  0.012439276
loss  138  :  0.012871405
loss  139  :  0.013136852
loss  140  :  0.014743048
loss  141  :  0.011089971
loss  142  :  0.0129164485
loss  143  :  0.009249088
loss  144  :  0.010284356
loss  145  :  0.013978062
loss  146  :  0.011802179
loss  147  :  0.015104154
loss  148  :  0.010738233
loss  149  :  0.009162502
loss  150  :  0.013424563
loss  151  :  0.009691782
loss  152  :  0.011314834
loss  153  :  0.009880665
loss  154  :  0.012645555
loss  155  :  0.014340076
loss  156  :  0.010714924
loss  157  :  0.013108036
loss  158  :  0.011532227
loss  159  :  0.011704122
loss  160  :  0.011129004
loss  161  :  0.009993403
loss  162  :  0.012208919
loss  163  :  0.014186757
loss  164  :  0.010234436
loss  165  :  0.013416561
loss  166  :  0.01154199
loss  167  :  0.010298072
loss  168  :  0.010889746
loss  169  :  0.014463115
loss  170  :  0.010841122
loss  171  :  0.01138779
loss  172  :  0.012207106
loss  173  :  0.01586011
loss  174  :  0.009104621
loss  175  :  0.011579536
loss  176  :  0.009236048
loss  177  :  0.012878174
loss  178  :  0.010190438
loss  179  :  0.010769325
loss  180  :  0.010643795
loss  181  :  0.010613151
loss  182  :  0.009118748
loss  183  :  0.01022494
loss  184  :  0.011516561
loss  185  :  0.013669938
loss  186  :  0.012807066
loss  187  :  0.010994511
loss  188  :  0.011746755
loss  189  :  0.011935706
loss  190  :  0.012315568
loss  191  :  0.01099744
loss  192  :  0.0127348555
loss  193  :  0.012197491
loss  194  :  0.010870603
loss  195  :  0.010955271
loss  196  :  0.010357821
loss  197  :  0.014477961
loss  198  :  0.013333525
loss  199  :  0.012330821
loss  200  :  0.009916937
loss  201  :  0.012701484
loss  202  :  0.010391322
loss  203  :  0.011605474
loss  204  :  0.010182589
loss  205  :  0.010750184
loss  206  :  0.010327244
loss  207  :  0.014960781
loss  208  :  0.014809209
loss  209  :  0.013567659
loss  210  :  0.011290629
loss  211  :  0.011680765
loss  212  :  0.012144033
loss  213  :  0.010066755
loss  214  :  0.012238033
loss  215  :  0.00933765
loss  216  :  0.00982301
loss  217  :  0.012067713
loss  218  :  0.01303305
loss  219  :  0.011525226
loss  220  :  0.0112578515
loss  221  :  0.010474717
loss  222  :  0.010411052
loss  223  :  0.011311759
loss  224  :  0.013152006
loss  225  :  0.010561602
loss  226  :  0.010728549
loss  227  :  0.009301262
loss  228  :  0.011231666
loss  229  :  0.008948985
loss  230  :  0.0130622685
loss  231  :  0.009328457
loss  232  :  0.009659322
loss  233  :  0.008799916
loss  234  :  0.0120946
loss  235  :  0.0132559445
loss  236  :  0.012208319
loss  237  :  0.011302544
loss  238  :  0.012933779
loss  239  :  0.010913961
loss  240  :  0.011582436
loss  241  :  0.013264927
loss  242  :  0.011766253
loss  243  :  0.011408165
loss  244  :  0.009029461
loss  245  :  0.011664485
loss  246  :  0.010201331
loss  247  :  0.011165617
loss  248  :  0.010612023
loss  249  :  0.011370939
loss  250  :  0.010215588
loss  251  :  0.008768851
loss  252  :  0.012601873
loss  253  :  0.009944631
loss  254  :  0.01183076
loss  255  :  0.011129685
loss  256  :  0.014632558
loss  257  :  0.008329678
loss  258  :  0.011421706
loss  259  :  0.013814146
paths_onpol:  0  running.....
data buffer size:  110000
total return:  1128.7676951296569
costs:  -1301.6448823693454
paths_onpol:  1  running.....
data buffer size:  111000
total return:  1036.1988037328426
costs:  -1200.2035973671634
paths_onpol:  2  running.....
data buffer size:  112000
total return:  992.9244697133848
costs:  -1085.3766181803796
paths_onpol:  3  running.....
data buffer size:  113000
total return:  1105.1841004450087
costs:  -1237.0186065620287
paths_onpol:  4  running.....
data buffer size:  114000
total return:  1129.4898168664286
costs:  -1298.8256390535335
paths_onpol:  5  running.....
data buffer size:  115000
total return:  1061.2708567062991
costs:  -1203.2109566512884
paths_onpol:  6  running.....
data buffer size:  116000
total return:  1085.9364695821803
costs:  -1220.346126553647
paths_onpol:  7  running.....
data buffer size:  117000
total return:  1074.9492266586178
costs:  -1189.4882810511383
paths_onpol:  8  running.....
data buffer size:  118000
total return:  1080.4420245094582
costs:  -1246.8358241397725
paths_onpol:  9  running.....
data buffer size:  119000
total return:  1124.5030783302368
costs:  -1280.0826423455192
-------------------------------------
|       Iteration |              10 |
|     AverageCost |       -1.23e+03 |
|         StdCost |            60.7 |
|     MinimumCost |        -1.3e+03 |
|     MaximumCost |       -1.09e+03 |
|   AverageReturn |        1.08e+03 |
|       StdReturn |            41.6 |
|   MinimumReturn |             993 |
|   MaximumReturn |        1.13e+03 |
-------------------------------------
onpol_iters:  11
Model fitting for  260 times ... 
loss  0  :  0.010946313
loss  1  :  0.012114232
loss  2  :  0.011263752
loss  3  :  0.012226941
loss  4  :  0.010823404
loss  5  :  0.011233669
loss  6  :  0.010771746
loss  7  :  0.011025479
loss  8  :  0.010628762
loss  9  :  0.014530388
loss  10  :  0.010673298
loss  11  :  0.014529774
loss  12  :  0.012809863
loss  13  :  0.013951125
loss  14  :  0.011413624
loss  15  :  0.010537127
loss  16  :  0.014675682
loss  17  :  0.012014655
loss  18  :  0.01339404
loss  19  :  0.008831203
loss  20  :  0.012330721
loss  21  :  0.010090923
loss  22  :  0.013333643
loss  23  :  0.010545274
loss  24  :  0.009582894
loss  25  :  0.012854746
loss  26  :  0.014086267
loss  27  :  0.010195257
loss  28  :  0.010907278
loss  29  :  0.012328821
loss  30  :  0.010440318
loss  31  :  0.010488962
loss  32  :  0.0110540055
loss  33  :  0.015786707
loss  34  :  0.012161804
loss  35  :  0.015183806
loss  36  :  0.011093375
loss  37  :  0.012054998
loss  38  :  0.010827819
loss  39  :  0.010430086
loss  40  :  0.010769354
loss  41  :  0.013388631
loss  42  :  0.011182568
loss  43  :  0.012994734
loss  44  :  0.0118392315
loss  45  :  0.01283662
loss  46  :  0.012288476
loss  47  :  0.009409514
loss  48  :  0.012341717
loss  49  :  0.01222424
loss  50  :  0.009799391
loss  51  :  0.012189654
loss  52  :  0.010083202
loss  53  :  0.0132591305
loss  54  :  0.014102372
loss  55  :  0.01136093
loss  56  :  0.01076133
loss  57  :  0.01083643
loss  58  :  0.012355503
loss  59  :  0.011698366
loss  60  :  0.010662546
loss  61  :  0.009047595
loss  62  :  0.014638225
loss  63  :  0.010620102
loss  64  :  0.0102722775
loss  65  :  0.010770355
loss  66  :  0.010206701
loss  67  :  0.01047373
loss  68  :  0.011354613
loss  69  :  0.010719439
loss  70  :  0.00921808
loss  71  :  0.0095909685
loss  72  :  0.010481644
loss  73  :  0.013327634
loss  74  :  0.011075677
loss  75  :  0.013752475
loss  76  :  0.00777656
loss  77  :  0.010404926
loss  78  :  0.011689765
loss  79  :  0.010661887
loss  80  :  0.011603296
loss  81  :  0.012360746
loss  82  :  0.0125338035
loss  83  :  0.010780846
loss  84  :  0.011463372
loss  85  :  0.015988404
loss  86  :  0.011527666
loss  87  :  0.010644389
loss  88  :  0.0110522825
loss  89  :  0.011588639
loss  90  :  0.011722043
loss  91  :  0.012497274
loss  92  :  0.010143808
loss  93  :  0.01099429
loss  94  :  0.010918606
loss  95  :  0.013830386
loss  96  :  0.015644431
loss  97  :  0.013639808
loss  98  :  0.0107093025
loss  99  :  0.009375297
loss  100  :  0.011235537
loss  101  :  0.016547922
loss  102  :  0.015429181
loss  103  :  0.011592476
loss  104  :  0.012172123
loss  105  :  0.010491001
loss  106  :  0.010612494
loss  107  :  0.01145423
loss  108  :  0.011422144
loss  109  :  0.011536621
loss  110  :  0.009948624
loss  111  :  0.01123734
loss  112  :  0.009080717
loss  113  :  0.015288785
loss  114  :  0.009698302
loss  115  :  0.010782212
loss  116  :  0.010757997
loss  117  :  0.010254848
loss  118  :  0.013019907
loss  119  :  0.011246251
loss  120  :  0.012258375
loss  121  :  0.01117752
loss  122  :  0.009666794
loss  123  :  0.012626404
loss  124  :  0.009826872
loss  125  :  0.0102078365
loss  126  :  0.00940865
loss  127  :  0.011466777
loss  128  :  0.013187143
loss  129  :  0.009250708
loss  130  :  0.011906813
loss  131  :  0.010286647
loss  132  :  0.009856475
loss  133  :  0.011901235
loss  134  :  0.010355806
loss  135  :  0.010922169
loss  136  :  0.011249044
loss  137  :  0.015620908
loss  138  :  0.0109826205
loss  139  :  0.01075902
loss  140  :  0.013213856
loss  141  :  0.010233721
loss  142  :  0.009438381
loss  143  :  0.011507104
loss  144  :  0.010728473
loss  145  :  0.009556139
loss  146  :  0.010922792
loss  147  :  0.011745812
loss  148  :  0.010753925
loss  149  :  0.009590269
loss  150  :  0.013571096
loss  151  :  0.008388935
loss  152  :  0.012223257
loss  153  :  0.01088497
loss  154  :  0.00945888
loss  155  :  0.008441066
loss  156  :  0.011339052
loss  157  :  0.009307054
loss  158  :  0.01115735
loss  159  :  0.008223963
loss  160  :  0.012548357
loss  161  :  0.011180142
loss  162  :  0.010769373
loss  163  :  0.009302322
loss  164  :  0.012017963
loss  165  :  0.00990554
loss  166  :  0.012358656
loss  167  :  0.016265742
loss  168  :  0.012067085
loss  169  :  0.010962319
loss  170  :  0.00938808
loss  171  :  0.008619225
loss  172  :  0.010063359
loss  173  :  0.014632886
loss  174  :  0.010922633
loss  175  :  0.011089924
loss  176  :  0.0095964465
loss  177  :  0.010016868
loss  178  :  0.009689657
loss  179  :  0.00861342
loss  180  :  0.009019828
loss  181  :  0.008231181
loss  182  :  0.009401323
loss  183  :  0.010869401
loss  184  :  0.010039626
loss  185  :  0.012197427
loss  186  :  0.009803626
loss  187  :  0.0091281785
loss  188  :  0.008853538
loss  189  :  0.011950931
loss  190  :  0.009874873
loss  191  :  0.0111473575
loss  192  :  0.011167534
loss  193  :  0.011128107
loss  194  :  0.01278835
loss  195  :  0.00843668
loss  196  :  0.011996443
loss  197  :  0.008802744
loss  198  :  0.00983236
loss  199  :  0.009658448
loss  200  :  0.010953851
loss  201  :  0.012615266
loss  202  :  0.008474202
loss  203  :  0.011806408
loss  204  :  0.00838002
loss  205  :  0.01056108
loss  206  :  0.010192196
loss  207  :  0.014634101
loss  208  :  0.009676082
loss  209  :  0.009540632
loss  210  :  0.010800707
loss  211  :  0.009275613
loss  212  :  0.01269912
loss  213  :  0.011004889
loss  214  :  0.011565156
loss  215  :  0.0094356565
loss  216  :  0.011931723
loss  217  :  0.01375854
loss  218  :  0.012829992
loss  219  :  0.011226153
loss  220  :  0.009515363
loss  221  :  0.015236293
loss  222  :  0.008476014
loss  223  :  0.011977318
loss  224  :  0.010945821
loss  225  :  0.010827569
loss  226  :  0.01347282
loss  227  :  0.010304353
loss  228  :  0.01197622
loss  229  :  0.013209094
loss  230  :  0.009807686
loss  231  :  0.012424755
loss  232  :  0.010792929
loss  233  :  0.009975206
loss  234  :  0.012541604
loss  235  :  0.011322655
loss  236  :  0.010609535
loss  237  :  0.0083445925
loss  238  :  0.011494462
loss  239  :  0.009321702
loss  240  :  0.0100799175
loss  241  :  0.009797427
loss  242  :  0.013415542
loss  243  :  0.014164346
loss  244  :  0.009567732
loss  245  :  0.013241695
loss  246  :  0.009205886
loss  247  :  0.011521498
loss  248  :  0.009161168
loss  249  :  0.010963823
loss  250  :  0.010870796
loss  251  :  0.008607283
loss  252  :  0.009090403
loss  253  :  0.012746425
loss  254  :  0.01046318
loss  255  :  0.009833565
loss  256  :  0.009493256
loss  257  :  0.009739333
loss  258  :  0.010077998
loss  259  :  0.009032617
paths_onpol:  0  running.....
data buffer size:  120000
total return:  1206.0883592457587
costs:  -1391.4410479341916
paths_onpol:  1  running.....
data buffer size:  121000
total return:  1112.96573516302
costs:  -1281.6734460013674
paths_onpol:  2  running.....
data buffer size:  122000
total return:  1122.7582998147345
costs:  -1287.123269677712
paths_onpol:  3  running.....
data buffer size:  123000
total return:  1107.7567110668792
costs:  -1271.928178592057
paths_onpol:  4  running.....
data buffer size:  124000
total return:  1116.9201154370191
costs:  -1254.112060102805
paths_onpol:  5  running.....
data buffer size:  125000
total return:  1103.102871552316
costs:  -1251.8071477895305
paths_onpol:  6  running.....
data buffer size:  126000
total return:  1112.9830843882824
costs:  -1270.0746888642145
paths_onpol:  7  running.....
data buffer size:  127000
total return:  1048.7307101442368
costs:  -1157.5717091512706
paths_onpol:  8  running.....
data buffer size:  128000
total return:  924.9932934016153
costs:  -1059.9004490646319
paths_onpol:  9  running.....
data buffer size:  129000
total return:  1078.0376858777736
costs:  -1234.3570092119476
-------------------------------------
|       Iteration |              11 |
|     AverageCost |       -1.25e+03 |
|         StdCost |            82.5 |
|     MinimumCost |       -1.39e+03 |
|     MaximumCost |       -1.06e+03 |
|   AverageReturn |        1.09e+03 |
|       StdReturn |            67.7 |
|   MinimumReturn |             925 |
|   MaximumReturn |        1.21e+03 |
-------------------------------------
onpol_iters:  12
Model fitting for  260 times ... 
loss  0  :  0.008987097
loss  1  :  0.009975856
loss  2  :  0.011782871
loss  3  :  0.011134996
loss  4  :  0.010620156
loss  5  :  0.011117502
loss  6  :  0.010074699
loss  7  :  0.008233683
loss  8  :  0.009093983
loss  9  :  0.009780379
loss  10  :  0.009846687
loss  11  :  0.011185566
loss  12  :  0.009774132
loss  13  :  0.008970023
loss  14  :  0.008970362
loss  15  :  0.009631056
loss  16  :  0.011030473
loss  17  :  0.0110591715
loss  18  :  0.009257505
loss  19  :  0.009937154
loss  20  :  0.009942688
loss  21  :  0.0101181
loss  22  :  0.010200384
loss  23  :  0.010907107
loss  24  :  0.013166745
loss  25  :  0.010799629
loss  26  :  0.008892926
loss  27  :  0.01148896
loss  28  :  0.010095729
loss  29  :  0.009136155
loss  30  :  0.009138929
loss  31  :  0.012447195
loss  32  :  0.009909952
loss  33  :  0.010033839
loss  34  :  0.008992839
loss  35  :  0.010310185
loss  36  :  0.010197357
loss  37  :  0.011381775
loss  38  :  0.010182789
loss  39  :  0.010569929
loss  40  :  0.010609308
loss  41  :  0.011139262
loss  42  :  0.011908698
loss  43  :  0.011547549
loss  44  :  0.010389451
loss  45  :  0.008320814
loss  46  :  0.012741801
loss  47  :  0.008389664
loss  48  :  0.009690021
loss  49  :  0.009763895
loss  50  :  0.013370318
loss  51  :  0.009164207
loss  52  :  0.009024641
loss  53  :  0.010340106
loss  54  :  0.011156138
loss  55  :  0.010483188
loss  56  :  0.012469819
loss  57  :  0.011011986
loss  58  :  0.0114558805
loss  59  :  0.010192566
loss  60  :  0.009626742
loss  61  :  0.008701915
loss  62  :  0.010962548
loss  63  :  0.012193997
loss  64  :  0.009583404
loss  65  :  0.01116519
loss  66  :  0.014538407
loss  67  :  0.011425738
loss  68  :  0.014655152
loss  69  :  0.010239732
loss  70  :  0.011750085
loss  71  :  0.009349945
loss  72  :  0.008722119
loss  73  :  0.009312105
loss  74  :  0.014003819
loss  75  :  0.010872295
loss  76  :  0.009340641
loss  77  :  0.013158587
loss  78  :  0.012979547
loss  79  :  0.010290755
loss  80  :  0.011070895
loss  81  :  0.009241002
loss  82  :  0.010147043
loss  83  :  0.0110447835
loss  84  :  0.009937675
loss  85  :  0.010530308
loss  86  :  0.014922962
loss  87  :  0.009498668
loss  88  :  0.009272726
loss  89  :  0.009737154
loss  90  :  0.009619729
loss  91  :  0.010858511
loss  92  :  0.008804574
loss  93  :  0.010486967
loss  94  :  0.009999007
loss  95  :  0.010931337
loss  96  :  0.010566726
loss  97  :  0.009420847
loss  98  :  0.011016378
loss  99  :  0.012718713
loss  100  :  0.01367604
loss  101  :  0.0101123545
loss  102  :  0.0155284405
loss  103  :  0.015766561
loss  104  :  0.012071088
loss  105  :  0.017895147
loss  106  :  0.010218342
loss  107  :  0.00924083
loss  108  :  0.009639583
loss  109  :  0.011487327
loss  110  :  0.01000272
loss  111  :  0.013666136
loss  112  :  0.01167847
loss  113  :  0.012139073
loss  114  :  0.010921609
loss  115  :  0.011884969
loss  116  :  0.009977063
loss  117  :  0.011113143
loss  118  :  0.010689892
loss  119  :  0.009285836
loss  120  :  0.012820683
loss  121  :  0.009581581
loss  122  :  0.010549344
loss  123  :  0.010964068
loss  124  :  0.01127159
loss  125  :  0.011983417
loss  126  :  0.009088197
loss  127  :  0.013072262
loss  128  :  0.008552293
loss  129  :  0.009249951
loss  130  :  0.010224132
loss  131  :  0.010135089
loss  132  :  0.0099823335
loss  133  :  0.011264821
loss  134  :  0.009791852
loss  135  :  0.012016279
loss  136  :  0.009754865
loss  137  :  0.011036521
loss  138  :  0.009157228
loss  139  :  0.012264948
loss  140  :  0.009325178
loss  141  :  0.009794814
loss  142  :  0.009262534
loss  143  :  0.009277092
loss  144  :  0.011011429
loss  145  :  0.011600817
loss  146  :  0.011184583
loss  147  :  0.010294812
loss  148  :  0.011387287
loss  149  :  0.011960333
loss  150  :  0.013209708
loss  151  :  0.010695303
loss  152  :  0.01092626
loss  153  :  0.0127817495
loss  154  :  0.009613062
loss  155  :  0.010358743
loss  156  :  0.011592577
loss  157  :  0.009671605
loss  158  :  0.008925749
loss  159  :  0.010552191
loss  160  :  0.012734616
loss  161  :  0.012333588
loss  162  :  0.01065208
loss  163  :  0.009999698
loss  164  :  0.011841742
loss  165  :  0.009713587
loss  166  :  0.009214333
loss  167  :  0.010962493
loss  168  :  0.009718092
loss  169  :  0.009287538
loss  170  :  0.010873405
loss  171  :  0.008569896
loss  172  :  0.008261626
loss  173  :  0.009642122
loss  174  :  0.009169501
loss  175  :  0.012061683
loss  176  :  0.011198488
loss  177  :  0.00936552
loss  178  :  0.008192865
loss  179  :  0.008416864
loss  180  :  0.01171173
loss  181  :  0.009588967
loss  182  :  0.010524521
loss  183  :  0.008816967
loss  184  :  0.0094866445
loss  185  :  0.010834902
loss  186  :  0.010839738
loss  187  :  0.010865689
loss  188  :  0.01024653
loss  189  :  0.010541479
loss  190  :  0.011356145
loss  191  :  0.009675224
loss  192  :  0.009560886
loss  193  :  0.010641621
loss  194  :  0.008666797
loss  195  :  0.0075866776
loss  196  :  0.009964244
loss  197  :  0.007526702
loss  198  :  0.010891599
loss  199  :  0.012761858
loss  200  :  0.013241954
loss  201  :  0.012056907
loss  202  :  0.011207635
loss  203  :  0.012521222
loss  204  :  0.00860245
loss  205  :  0.012723787
loss  206  :  0.011490596
loss  207  :  0.0086746365
loss  208  :  0.012087084
loss  209  :  0.012550044
loss  210  :  0.010353771
loss  211  :  0.009531979
loss  212  :  0.009755593
loss  213  :  0.010393711
loss  214  :  0.009620714
loss  215  :  0.009677326
loss  216  :  0.009255245
loss  217  :  0.0074279876
loss  218  :  0.008111833
loss  219  :  0.010505079
loss  220  :  0.011741677
loss  221  :  0.009065292
loss  222  :  0.009011331
loss  223  :  0.015874283
loss  224  :  0.012236536
loss  225  :  0.010813582
loss  226  :  0.009583569
loss  227  :  0.012535137
loss  228  :  0.008165745
loss  229  :  0.010491429
loss  230  :  0.0110235745
loss  231  :  0.011403814
loss  232  :  0.011589174
loss  233  :  0.012985046
loss  234  :  0.00874947
loss  235  :  0.008313632
loss  236  :  0.009775606
loss  237  :  0.009618022
loss  238  :  0.007753673
loss  239  :  0.009608311
loss  240  :  0.010281362
loss  241  :  0.012465362
loss  242  :  0.012617645
loss  243  :  0.010858442
loss  244  :  0.007951795
loss  245  :  0.010450256
loss  246  :  0.009642909
loss  247  :  0.008666364
loss  248  :  0.012246682
loss  249  :  0.010167247
loss  250  :  0.0076619536
loss  251  :  0.013129756
loss  252  :  0.009778423
loss  253  :  0.01048368
loss  254  :  0.011880139
loss  255  :  0.010636747
loss  256  :  0.009993283
loss  257  :  0.011632478
loss  258  :  0.011744903
loss  259  :  0.011000017
paths_onpol:  0  running.....
data buffer size:  130000
total return:  1005.2344869657992
costs:  -1172.7694870310588
paths_onpol:  1  running.....
data buffer size:  131000
total return:  1018.708267760156
costs:  -1203.178171541708
paths_onpol:  2  running.....
data buffer size:  132000
total return:  992.4527012015545
costs:  -1140.6378203868267
paths_onpol:  3  running.....
data buffer size:  133000
total return:  940.2810901542616
costs:  -1104.4204119006702
paths_onpol:  4  running.....
data buffer size:  134000
total return:  959.2620713531129
costs:  -1116.0273529289364
paths_onpol:  5  running.....
data buffer size:  135000
total return:  1070.8599289780527
costs:  -1238.7381545180713
paths_onpol:  6  running.....
data buffer size:  136000
total return:  993.3917880859941
costs:  -1197.9880078842896
paths_onpol:  7  running.....
data buffer size:  137000
total return:  951.7584867951263
costs:  -1088.7139355495713
paths_onpol:  8  running.....
data buffer size:  138000
total return:  937.947390989456
costs:  -1095.754337179572
paths_onpol:  9  running.....
data buffer size:  139000
total return:  1032.3757081274255
costs:  -1200.12550167649
-------------------------------------
|       Iteration |              12 |
|     AverageCost |       -1.16e+03 |
|         StdCost |            50.7 |
|     MinimumCost |       -1.24e+03 |
|     MaximumCost |       -1.09e+03 |
|   AverageReturn |             990 |
|       StdReturn |            41.2 |
|   MinimumReturn |             938 |
|   MaximumReturn |        1.07e+03 |
-------------------------------------
onpol_iters:  13
Model fitting for  260 times ... 
loss  0  :  0.0095568355
loss  1  :  0.009548895
loss  2  :  0.009607121
loss  3  :  0.0077282107
loss  4  :  0.011357841
loss  5  :  0.010146225
loss  6  :  0.009017016
loss  7  :  0.0090110395
loss  8  :  0.011013063
loss  9  :  0.009208113
loss  10  :  0.010623092
loss  11  :  0.009720853
loss  12  :  0.008966892
loss  13  :  0.00963814
loss  14  :  0.011394055
loss  15  :  0.009179741
loss  16  :  0.010725304
loss  17  :  0.011196628
loss  18  :  0.010615257
loss  19  :  0.009371515
loss  20  :  0.008572681
loss  21  :  0.010208828
loss  22  :  0.01036632
loss  23  :  0.008436919
loss  24  :  0.009272845
loss  25  :  0.008495114
loss  26  :  0.009630832
loss  27  :  0.008961516
loss  28  :  0.013405988
loss  29  :  0.010229832
loss  30  :  0.008187546
loss  31  :  0.009912724
loss  32  :  0.0102622295
loss  33  :  0.007563129
loss  34  :  0.012998973
loss  35  :  0.009668417
loss  36  :  0.0093172435
loss  37  :  0.009549702
loss  38  :  0.008601325
loss  39  :  0.009250654
loss  40  :  0.008061933
loss  41  :  0.008963625
loss  42  :  0.01186479
loss  43  :  0.009839766
loss  44  :  0.008894118
loss  45  :  0.009697191
loss  46  :  0.008164944
loss  47  :  0.007474442
loss  48  :  0.011224585
loss  49  :  0.009313377
loss  50  :  0.008924625
loss  51  :  0.012145692
loss  52  :  0.00891788
loss  53  :  0.00889938
loss  54  :  0.010214619
loss  55  :  0.009820112
loss  56  :  0.008437822
loss  57  :  0.008448039
loss  58  :  0.00789764
loss  59  :  0.0095528355
loss  60  :  0.009645636
loss  61  :  0.010532418
loss  62  :  0.0107411565
loss  63  :  0.009564781
loss  64  :  0.010184996
loss  65  :  0.009672866
loss  66  :  0.012084371
loss  67  :  0.010008162
loss  68  :  0.0084903035
loss  69  :  0.013459535
loss  70  :  0.0077656396
loss  71  :  0.010025976
loss  72  :  0.011208258
loss  73  :  0.011472066
loss  74  :  0.011508591
loss  75  :  0.009326894
loss  76  :  0.01065903
loss  77  :  0.008312874
loss  78  :  0.0096185105
loss  79  :  0.011010434
loss  80  :  0.009490438
loss  81  :  0.009172468
loss  82  :  0.0074413074
loss  83  :  0.009972371
loss  84  :  0.012768023
loss  85  :  0.008863667
loss  86  :  0.010720667
loss  87  :  0.0087389285
loss  88  :  0.008492291
loss  89  :  0.009668587
loss  90  :  0.008704481
loss  91  :  0.010620631
loss  92  :  0.0108026955
loss  93  :  0.009400579
loss  94  :  0.0089027155
loss  95  :  0.011919606
loss  96  :  0.0085004885
loss  97  :  0.009216329
loss  98  :  0.013409319
loss  99  :  0.009420226
loss  100  :  0.007895374
loss  101  :  0.009549502
loss  102  :  0.010265142
loss  103  :  0.009125754
loss  104  :  0.009888682
loss  105  :  0.008988422
loss  106  :  0.007537274
loss  107  :  0.009760089
loss  108  :  0.008786628
loss  109  :  0.011541225
loss  110  :  0.008731486
loss  111  :  0.010139594
loss  112  :  0.008540631
loss  113  :  0.009592279
loss  114  :  0.007966481
loss  115  :  0.010492494
loss  116  :  0.01259906
loss  117  :  0.009324392
loss  118  :  0.012531261
loss  119  :  0.009756526
loss  120  :  0.009830509
loss  121  :  0.013644597
loss  122  :  0.011723223
loss  123  :  0.009402024
loss  124  :  0.00984941
loss  125  :  0.009199791
loss  126  :  0.009355115
loss  127  :  0.008911558
loss  128  :  0.0081006875
loss  129  :  0.010026473
loss  130  :  0.010086102
loss  131  :  0.0110170115
loss  132  :  0.008374833
loss  133  :  0.011477562
loss  134  :  0.010376727
loss  135  :  0.010518142
loss  136  :  0.011570489
loss  137  :  0.012427079
loss  138  :  0.010573305
loss  139  :  0.010484574
loss  140  :  0.010408423
loss  141  :  0.01171211
loss  142  :  0.008088062
loss  143  :  0.008835131
loss  144  :  0.008657741
loss  145  :  0.010055601
loss  146  :  0.008803788
loss  147  :  0.009449495
loss  148  :  0.008801018
loss  149  :  0.011773479
loss  150  :  0.0103769675
loss  151  :  0.007765503
loss  152  :  0.008592909
loss  153  :  0.010594216
loss  154  :  0.009131059
loss  155  :  0.013281305
loss  156  :  0.010439826
loss  157  :  0.012342645
loss  158  :  0.009106539
loss  159  :  0.010664569
loss  160  :  0.0076493644
loss  161  :  0.01104379
loss  162  :  0.009743035
loss  163  :  0.012044929
loss  164  :  0.011363311
loss  165  :  0.008637336
loss  166  :  0.009276664
loss  167  :  0.007834012
loss  168  :  0.009498755
loss  169  :  0.00951696
loss  170  :  0.009975964
loss  171  :  0.010766974
loss  172  :  0.008947438
loss  173  :  0.01034371
loss  174  :  0.011382446
loss  175  :  0.011030247
loss  176  :  0.009726557
loss  177  :  0.008442785
loss  178  :  0.0083915945
loss  179  :  0.010772076
loss  180  :  0.00817508
loss  181  :  0.013612455
loss  182  :  0.011432091
loss  183  :  0.0092133675
loss  184  :  0.010277
loss  185  :  0.008990333
loss  186  :  0.008608872
loss  187  :  0.009495914
loss  188  :  0.0086809145
loss  189  :  0.009188853
loss  190  :  0.007475503
loss  191  :  0.009992853
loss  192  :  0.00968988
loss  193  :  0.010934526
loss  194  :  0.011482139
loss  195  :  0.010915056
loss  196  :  0.009996681
loss  197  :  0.011453365
loss  198  :  0.011909316
loss  199  :  0.008518746
loss  200  :  0.009705683
loss  201  :  0.009515622
loss  202  :  0.008328563
loss  203  :  0.009603219
loss  204  :  0.008895008
loss  205  :  0.009375852
loss  206  :  0.01028368
loss  207  :  0.00987335
loss  208  :  0.010853441
loss  209  :  0.007943501
loss  210  :  0.009391621
loss  211  :  0.009198146
loss  212  :  0.009738175
loss  213  :  0.010209302
loss  214  :  0.008887067
loss  215  :  0.009687524
loss  216  :  0.013372719
loss  217  :  0.013425824
loss  218  :  0.009101808
loss  219  :  0.0114445435
loss  220  :  0.011289978
loss  221  :  0.011190215
loss  222  :  0.009443188
loss  223  :  0.010248837
loss  224  :  0.010888422
loss  225  :  0.009914799
loss  226  :  0.008250669
loss  227  :  0.007215254
loss  228  :  0.009986441
loss  229  :  0.010907542
loss  230  :  0.012186108
loss  231  :  0.009519847
loss  232  :  0.009398093
loss  233  :  0.00755524
loss  234  :  0.007898447
loss  235  :  0.009165475
loss  236  :  0.011235532
loss  237  :  0.008278692
loss  238  :  0.009827791
loss  239  :  0.00978333
loss  240  :  0.008198741
loss  241  :  0.0071082846
loss  242  :  0.009288982
loss  243  :  0.009091666
loss  244  :  0.012117508
loss  245  :  0.0090530515
loss  246  :  0.008055118
loss  247  :  0.009294219
loss  248  :  0.011403081
loss  249  :  0.009344414
loss  250  :  0.010138536
loss  251  :  0.009531826
loss  252  :  0.010045113
loss  253  :  0.008915538
loss  254  :  0.009875665
loss  255  :  0.007276059
loss  256  :  0.0074955234
loss  257  :  0.009755297
loss  258  :  0.007816598
loss  259  :  0.008759924
paths_onpol:  0  running.....
data buffer size:  140000
total return:  1056.8786581398742
costs:  -1175.187563195095
paths_onpol:  1  running.....
data buffer size:  141000
total return:  1061.8907406091762
costs:  -1250.196289434595
paths_onpol:  2  running.....
data buffer size:  142000
total return:  1049.923891580508
costs:  -1196.3222885520636
paths_onpol:  3  running.....
data buffer size:  143000
total return:  974.6495672834552
costs:  -1169.5676363760324
paths_onpol:  4  running.....
data buffer size:  144000
total return:  1046.4972966746354
costs:  -1196.1918625667279
paths_onpol:  5  running.....
data buffer size:  145000
total return:  967.3661265609607
costs:  -1126.1971349264586
paths_onpol:  6  running.....
data buffer size:  146000
total return:  1062.0617622047819
costs:  -1139.3529026088838
paths_onpol:  7  running.....
data buffer size:  147000
total return:  1029.39565374051
costs:  -1154.4637329992186
paths_onpol:  8  running.....
data buffer size:  148000
total return:  1003.3920316049126
costs:  -1116.8226496183518
paths_onpol:  9  running.....
data buffer size:  149000
total return:  1026.311349944867
costs:  -1186.8407397040555
-------------------------------------
|       Iteration |              13 |
|     AverageCost |       -1.17e+03 |
|         StdCost |            37.5 |
|     MinimumCost |       -1.25e+03 |
|     MaximumCost |       -1.12e+03 |
|   AverageReturn |        1.03e+03 |
|       StdReturn |            33.3 |
|   MinimumReturn |             967 |
|   MaximumReturn |        1.06e+03 |
-------------------------------------
onpol_iters:  14
Model fitting for  260 times ... 
loss  0  :  0.00790578
loss  1  :  0.011107587
loss  2  :  0.009651085
loss  3  :  0.008109666
loss  4  :  0.00895454
loss  5  :  0.0105676
loss  6  :  0.009437238
loss  7  :  0.011660511
loss  8  :  0.009188874
loss  9  :  0.009645057
loss  10  :  0.009796994
loss  11  :  0.008941961
loss  12  :  0.008522842
loss  13  :  0.008547049
loss  14  :  0.013257692
loss  15  :  0.011405504
loss  16  :  0.0077155293
loss  17  :  0.009090578
loss  18  :  0.010324575
loss  19  :  0.007966777
loss  20  :  0.008684702
loss  21  :  0.008679485
loss  22  :  0.008898733
loss  23  :  0.0074930377
loss  24  :  0.008198437
loss  25  :  0.008584034
loss  26  :  0.009044689
loss  27  :  0.0075939163
loss  28  :  0.012249004
loss  29  :  0.0094682975
loss  30  :  0.010280125
loss  31  :  0.010527757
loss  32  :  0.007864087
loss  33  :  0.008320907
loss  34  :  0.007920766
loss  35  :  0.009209096
loss  36  :  0.009739183
loss  37  :  0.009288119
loss  38  :  0.008336497
loss  39  :  0.008582188
loss  40  :  0.008680491
loss  41  :  0.009278343
loss  42  :  0.009769641
loss  43  :  0.008525362
loss  44  :  0.008512934
loss  45  :  0.009342964
loss  46  :  0.009027898
loss  47  :  0.0071972995
loss  48  :  0.008071151
loss  49  :  0.007916081
loss  50  :  0.009548753
loss  51  :  0.009449294
loss  52  :  0.009250748
loss  53  :  0.008148645
loss  54  :  0.01101054
loss  55  :  0.00903666
loss  56  :  0.010671338
loss  57  :  0.008360131
loss  58  :  0.0071392097
loss  59  :  0.008545345
loss  60  :  0.009690237
loss  61  :  0.0083316285
loss  62  :  0.008049992
loss  63  :  0.008829644
loss  64  :  0.007921163
loss  65  :  0.009414478
loss  66  :  0.008469045
loss  67  :  0.008239659
loss  68  :  0.0096071055
loss  69  :  0.0090266885
loss  70  :  0.007137639
loss  71  :  0.009986307
loss  72  :  0.008430214
loss  73  :  0.00843309
loss  74  :  0.009796863
loss  75  :  0.007819103
loss  76  :  0.008786625
loss  77  :  0.010509701
loss  78  :  0.008898777
loss  79  :  0.008639198
loss  80  :  0.008018525
loss  81  :  0.008975571
loss  82  :  0.009222565
loss  83  :  0.008231046
loss  84  :  0.010163038
loss  85  :  0.008324574
loss  86  :  0.00973385
loss  87  :  0.008830266
loss  88  :  0.0075398684
loss  89  :  0.009816535
loss  90  :  0.009785002
loss  91  :  0.008735284
loss  92  :  0.009135282
loss  93  :  0.008030698
loss  94  :  0.009392502
loss  95  :  0.008287297
loss  96  :  0.009270059
loss  97  :  0.008183343
loss  98  :  0.007702835
loss  99  :  0.008722737
loss  100  :  0.0082887905
loss  101  :  0.0071898727
loss  102  :  0.00707171
loss  103  :  0.008265068
loss  104  :  0.0064255
loss  105  :  0.0075935186
loss  106  :  0.0074769957
loss  107  :  0.010472594
loss  108  :  0.008361598
loss  109  :  0.0069492282
loss  110  :  0.008190951
loss  111  :  0.007521416
loss  112  :  0.010019721
loss  113  :  0.009127569
loss  114  :  0.008123843
loss  115  :  0.008903581
loss  116  :  0.007937786
loss  117  :  0.006291817
loss  118  :  0.0065308213
loss  119  :  0.008642493
loss  120  :  0.0075562857
loss  121  :  0.007276668
loss  122  :  0.0074524907
loss  123  :  0.009998171
loss  124  :  0.009399888
loss  125  :  0.008184341
loss  126  :  0.011557835
loss  127  :  0.010492882
loss  128  :  0.009026331
loss  129  :  0.008837824
loss  130  :  0.011139835
loss  131  :  0.008130664
loss  132  :  0.008983529
loss  133  :  0.008332069
loss  134  :  0.013004142
loss  135  :  0.008875042
loss  136  :  0.009107376
loss  137  :  0.010068681
loss  138  :  0.009300111
loss  139  :  0.0073596076
loss  140  :  0.0072702738
loss  141  :  0.010002626
loss  142  :  0.0084339585
loss  143  :  0.00828837
loss  144  :  0.008494298
loss  145  :  0.0075027915
loss  146  :  0.010208811
loss  147  :  0.009962104
loss  148  :  0.0075114765
loss  149  :  0.008782887
loss  150  :  0.011881737
loss  151  :  0.008340387
loss  152  :  0.006979257
loss  153  :  0.010490832
loss  154  :  0.008453248
loss  155  :  0.007465571
loss  156  :  0.008677124
loss  157  :  0.010127099
loss  158  :  0.0073594814
loss  159  :  0.008220775
loss  160  :  0.00838028
loss  161  :  0.0077568605
loss  162  :  0.008232197
loss  163  :  0.008322251
loss  164  :  0.0074453326
loss  165  :  0.011183715
loss  166  :  0.008784068
loss  167  :  0.008325321
loss  168  :  0.010412509
loss  169  :  0.010602536
loss  170  :  0.008728902
loss  171  :  0.008132051
loss  172  :  0.007960449
loss  173  :  0.0077072517
loss  174  :  0.0074209706
loss  175  :  0.008288383
loss  176  :  0.008445923
loss  177  :  0.007372172
loss  178  :  0.009616886
loss  179  :  0.0077485517
loss  180  :  0.008114507
loss  181  :  0.010691179
loss  182  :  0.0070192777
loss  183  :  0.011064386
loss  184  :  0.008794667
loss  185  :  0.008467063
loss  186  :  0.00797029
loss  187  :  0.010872781
loss  188  :  0.007867868
loss  189  :  0.008049504
loss  190  :  0.007521411
loss  191  :  0.010427742
loss  192  :  0.009913907
loss  193  :  0.010774593
loss  194  :  0.009078347
loss  195  :  0.0088843005
loss  196  :  0.007991433
loss  197  :  0.009813708
loss  198  :  0.008979315
loss  199  :  0.010480711
loss  200  :  0.0078226235
loss  201  :  0.008235132
loss  202  :  0.0076227845
loss  203  :  0.007842457
loss  204  :  0.009973889
loss  205  :  0.007847828
loss  206  :  0.008257588
loss  207  :  0.0076024733
loss  208  :  0.008251719
loss  209  :  0.0115773445
loss  210  :  0.008846415
loss  211  :  0.009804127
loss  212  :  0.010418003
loss  213  :  0.011619834
loss  214  :  0.008839244
loss  215  :  0.007854698
loss  216  :  0.009733054
loss  217  :  0.009413089
loss  218  :  0.009128952
loss  219  :  0.0075639114
loss  220  :  0.00818873
loss  221  :  0.008019541
loss  222  :  0.008351539
loss  223  :  0.0069943285
loss  224  :  0.0074877394
loss  225  :  0.009396168
loss  226  :  0.0094203595
loss  227  :  0.01068619
loss  228  :  0.008839147
loss  229  :  0.008839855
loss  230  :  0.010299437
loss  231  :  0.009297725
loss  232  :  0.008325789
loss  233  :  0.009593368
loss  234  :  0.008519556
loss  235  :  0.00901816
loss  236  :  0.00972729
loss  237  :  0.00795758
loss  238  :  0.0070442758
loss  239  :  0.00667355
loss  240  :  0.008962568
loss  241  :  0.0086541185
loss  242  :  0.009198013
loss  243  :  0.008437685
loss  244  :  0.009209412
loss  245  :  0.0075163557
loss  246  :  0.009173522
loss  247  :  0.006750127
loss  248  :  0.007796866
loss  249  :  0.00795802
loss  250  :  0.0074345903
loss  251  :  0.008061739
loss  252  :  0.00687572
loss  253  :  0.0076557607
loss  254  :  0.009731205
loss  255  :  0.0077962168
loss  256  :  0.0076617585
loss  257  :  0.008129711
loss  258  :  0.009236068
loss  259  :  0.009545227
paths_onpol:  0  running.....
data buffer size:  150000
total return:  1305.8061207969322
costs:  -1465.4574256707385
paths_onpol:  1  running.....
data buffer size:  151000
total return:  1211.8107143443008
costs:  -1360.457964747161
paths_onpol:  2  running.....
data buffer size: 