PPO

working lr:1e-4, 3e-4
working data size > 1000 updated data
working batch 64 128
optimization steps: ~320

act(stochastic) is very important for ppo along, without stochasticy it diverge!
lr 1e-4 learns slower but also more stable
Â·
BC:

working lr: 1e-4
working data size > 1000 cumulative data
working batch 64 128
optimization steps: ~2000-3000 (depends on how many data in buffer, if buffer is small use small optimization steps ~100 step for ~1000 data)

1 .Use MPC action as real action can work in the early stage up to ~800 real return, however bc can't follow (only ~300) since the real action distribution is not same as bc.

Try use bc action as real
Use all action from bc still can't increase bc (~400) there should be something wrong with BC it self

